{
  "AISettings": "AI Settings",
  "APIKeyInstruction": "You can obtain the API key below. Enter the obtained API key into the form.",
  "APIKeyNotEntered": "API key is not entered.",
  "AboutThisApplication": "About This Application",
  "AboutThisApplicationDescription": "Enjoy conversations with a 3D character right in your web browser, using microphone or text input and voice synthesis. You can also change the character (VRM), adjust its personality, and modify its voice.<br />Settings can be changed from the menu button in the top left.",
  "AboutThisApplicationDescription2": "If you want to change the character, please refer to the \"Character Settings\" tab.",
  "AivisSpeechInfo": "Using AivisSpeech. It only supports Japanese. It uses a local API, you need to download and launch the app that suits your environment from the site below.",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechPitch": "Pitch",
  "AivisSpeechServerUrl": "AivisSpeech Server URL",
  "AivisSpeechSpeaker": "Speaker",
  "AivisSpeechSpeed": "Speed",
  "AnswerGenerating": "Answer Generating",
  "AnthropicAPIKeyLabel": "Anthropic API Key",
  "AudioMode": "Audio Mode",
  "AuthFileInstruction": "API key or authentication file is required. Get it from the URL below and place it in the root folder of the repository if it is a JSON file.",
  "AzureAPIKeyLabel": "Azure OpenAI API Key",
  "AzureAPIURL": "Azure OpenAI API URL",
  "AzureEndpoint": "Azure Endpoint",
  "AzureTTSInfo": "Using Azure OpenAI. It supports multiple languages.",
  "BackgroundImage": "Background Image",
  "BackgroundSettings": "Background Settings",
  "BackgroundSettingsDescription": "You can upload and select a background image for the application.",
  "BasedSettings": "Basic Settings",
  "BrowserSpeechRecognition": "Use Browser Standard Speech Recognition",
  "CannotUseParameters": "If real-time API mode or audio mode is enabled, the Temperature and Max Tokens parameters cannot be specified.",
  "CannotUseVoice": "If real-time API mode or audio mode is enabled,\nSpeech synthesis settings are not needed.",
  "ChangeBackgroundImage": "Change Background Image",
  "CharacterModelInfo": "The model may take time to load when first displayed.",
  "CharacterModelLabel": "Character Model",
  "CharacterName": "Character name",
  "CharacterSettings": "Character Settings",
  "CharacterSettingsInfo": "This value is set as the system prompt.\nPlease refer to the initial prompt and specify the emotion tags to control the character's expressions and motions. Example: [neutral]Good morning![happy]Today is also a hard day!",
  "CharacterSettingsPrompt": "Character Prompt",
  "Characterpreset1": "Preset 1",
  "Characterpreset2": "Preset 2",
  "Characterpreset3": "Preset 3",
  "Characterpreset4": "Preset 4",
  "Characterpreset5": "Preset 5",
  "CharacterpresetInfo": "Selecting a preset will change the character prompt.\nYou can use shortcuts with Cmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows).",
  "ChatLog": "Conversation Log",
  "ClientID": "Client ID",
  "Close": "CLOSE",
  "CohereAPIKeyLabel": "Cohere API Key",
  "Contact": "Contact",
  "ContactDescription": "Please contact me via the email address or Twitter account below regarding this app.",
  "ContinuousMic": "Continuous Microphone Input",
  "ContinuousMicActive": "Continuous Microphone Input Active",
  "ContinuousMicInfo": "The microphone input will automatically resume when the AI's speech ends. It will automatically send after the set silence duration.\nIf the set time is exceeded without speech recognition, continuous microphone input will automatically turn OFF, so if you want it to always be ON, set the speech recognition timeout to 0 seconds.",
  "ContinuousMicModeOff": "Continuous Microphone Input Mode is Off",
  "ContinuousMicModeOn": "Continuous Microphone Input Mode is On",
  "ConversationContinuityMode": "Conversation Continuity Mode (Beta)",
  "ConversationContinuityModeInfo": "When there is no comment, AI tries to continue the conversation. Currently only OpenAI, Anthropic Claude, Google Gemini are supported.",
  "ConversationContinuityModeInfo2": "One answer calls LLM multiple times, so API usage may increase. Please be aware of this.",
  "ConversationContinuityModeInfo3": "gpt-4o, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet work relatively stably.",
  "ConversationHistory": "Conversation History",
  "ConversationHistoryInfo": "The last {{count}} conversation texts will be retained as memory.",
  "ConversationHistoryReset": "Reset Conversation History",
  "Creator": "Creator",
  "CreatorDescription": "Creator: Tegan",
  "CustomAPIBody": "Custom Body",
  "CustomAPIBodyInfo": "Please enter the body information to include in the API request in JSON format. Messages will be included automatically.",
  "CustomAPIDescription": "Note: Messages are automatically included in the request body. In streaming mode, the server must return text/event-stream.",
  "CustomAPIEndpoint": "Custom API Endpoint",
  "CustomAPIEndpointInfo": "Please enter the URL of the API endpoint to send POST requests.",
  "CustomAPIHeaders": "Custom Headers",
  "CustomAPIHeadersInfo": "Please enter the header information to include in the API request in JSON format.",
  "CustomAPIStream": "Streaming Mode",
  "CustomAPIStreamForced": "Currently, streaming mode is always enabled.",
  "CustomVoiceTextPlaceholder": "Enter text you want to hear",
  "DeepSeekAPIKeyLabel": "DeepSeek API Key",
  "DefaultBackground": "Default Background",
  "Description": "About the app",
  "DifyAPIKeyLabel": "Dify API Key",
  "DifyInfo": "Dify only supports chatbot and agent type.",
  "DifyInfo2": "The length of the conversation history is dependent on the specifications of Dify.",
  "DifyInfo3": "Example: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "If you are using Dify, the system prompt will not be used. Please set Dify chatbot.",
  "DocumentationDescription": "For detailed usage and tutorials of AITuberKit, please visit the URL below.",
  "DontShowIntroductionNextTime": "Do not show this dialog next time",
  "DragToReorder": "Drag to reorder",
  "ElevenLabsApiKey": "ElevenLabs API Key",
  "ElevenLabsInfo": "ElevenLabs API is used. It supports multiple languages. API key can be obtained from the URL below.",
  "ElevenLabsVoiceId": "ElevenLabs Voice ID",
  "ElevenLabsVoiceIdInfo": "Voice ID can be selected from the URL below.",
  "EnterPresetQuestion": "Please enter a question",
  "EnterURL": "URL",
  "EnterYourQuestion": "Enter your question here",
  "Errors": {
    "AIAPIError": "An error occurred while executing the AI API",
    "AIInvalidProperty": "AI service settings are incorrect",
    "CustomAPIError": "An error occurred with the custom API",
    "EmptyAPIKey": "API key is not set",
    "EmptyLocalLLMURL": "The local LLM URL is not set",
    "InvalidAIService": "The selected AI service is not valid",
    "InvalidJSON": "The JSON format is incorrect",
    "LocalLLMAPIError": "Local LLM API error",
    "LocalLLMConnectionError": "Local LLM server connection error",
    "LocalLLMError": "Local LLM error",
    "LocalLLMNotFound": "Local LLM endpoint not found",
    "LocalLLMStreamError": "Local LLM stream error",
    "MethodNotAllowed": "The request is not appropriate",
    "TTSServiceError": "An error occurred in the {{serviceName}} TTS service: {{message}}",
    "UnexpectedError": "An unexpected error occurred"
  },
  "ExternalLinkageMode": "External Linkage Mode (Beta)",
  "FireworksAPIKeyLabel": "Fireworks API Key",
  "GSVITTSBatchSize": "GSVI TTS Batch Size (1 ~ 100 The larger the value, the faster the inference speed, but it might exhaust memory if too large.)",
  "GSVITTSInfo": "GSVI TTS Settings",
  "GSVITTSModelID": "GSVI TTS Model ID",
  "GSVITTSServerUrl": "GSVI TTS Endpoint API",
  "GSVITTSSpeechRate": "Speech Rate (0.5 ~ 2.0 The bigger the value, the faster it is.)",
  "GoogleAPIKeyLabel": "Google Gemini API Key",
  "GoogleTTSInfo": "Using Google Cloud Text-to-Speech. It supports multiple languages.",
  "GroqAPIKeyLabel": "Groq API Key",
  "GroqInfo": "Groq API is accessed directly from the browser.",
  "IncludeTimestampInUserMessage": "Include timestamp in user message",
  "IncludeTimestampInUserMessageInfo": "By including timestamps in user messages, the AI can generate responses while considering the time.\nPlease include the following text in your system prompt:\n\n\"User input may include [timestamp]. This represents the UTC time at the moment of the request, so please generate responses considering this timestamp.\"",
  "InitialSpeechTimeout": "Speech Recognition Timeout",
  "InitialSpeechTimeoutInfo": "Set the waiting time until the first utterance is detected after speech recognition starts. If no utterance is detected within this time, speech recognition will automatically stop.\nSetting it to 0 seconds will make the waiting time unlimited.",
  "InputAudio": "Audio",
  "InputText": "Text",
  "KoeiromapInfo": "Using Koeiromap API from Koemotion. It only supports Japanese. For more details, please refer to the link below.",
  "Language": "Language",
  "LanguageChoice": "Language Choice",
  "LanguageModelURL": "Select the language model from the URL below.",
  "ListeningContinuously": "Waiting for voice input...",
  "Live2D": {
    "EmotionInfo": "Emotions can be specified in comma-separated format. If multiple emotions are specified, they are randomly selected.\nThe initial value is for the model provided by AITuberKit. If you are using an original model, please enter the value according to your model.\nAfter conversation completion, the \"Neutral\" emotion is displayed.",
    "Emotions": "Emotion Settings",
    "FileInfo": "Place the Live2D model you want to use in the public/live2d folder. The model3.json file must exist in the root of this folder.\nIf it is not displayed in the selection, please reload the screen or check if the folder path is correct.",
    "Info": "You can specify emotions and motions.\nEach emotion is controlled by the prompt. For more details, please refer to \"AI Settings => Character Settings\".",
    "MotionGroups": "Motion Group Settings",
    "MotionGroupsInfo": "Motion groups are randomly selected from the selected group.\nSame as emotion settings, please set it according to your model.\n\"Idle\" is the motion displayed after conversation completion.",
    "SelectMotionGroup": "Select Motion Group",
    "angryEmotions": "Angry",
    "angryMotionGroup": "Angry",
    "happyEmotions": "Happy",
    "happyMotionGroup": "Happy",
    "idleMotionGroup": "Idle",
    "neutralEmotions": "Neutral",
    "neutralMotionGroup": "Neutral",
    "relaxedEmotions": "Relaxed",
    "relaxedMotionGroup": "Relaxed",
    "sadEmotions": "Sad",
    "sadMotionGroup": "Sad",
    "surprisedEmotions": "Surprise",
    "surprisedMotionGroup": "Surprise"
  },
  "LocalLLM": "Local LLM",
  "LocalLLMInfo": "Local LLM server must be running. Setup is as follows.",
  "LocalLLMInfo2": "Please enter the URL of the local LLM server (including port number) and the model name.",
  "LocalStorageReset": "Reset Settings",
  "LocalStorageResetButton": "Reset Settings",
  "LocalStorageResetInfo": "Environment variables are prioritized if set. The page will be reloaded.",
  "LogSettings": "Conversation History",
  "MaxPastMessages": "Number of Past Messages to Keep",
  "MaxTokens": "Maximum Token Count",
  "MaxTokensInfo": "The maximum token count varies depending on the AI model in use. Please check the specifications of each model.",
  "MessageReceiver": "Receive instructions from outside",
  "MessageReceiverDescription": "You can use API to instruct AI characters to speak from outside.",
  "Milliseconds": "Milliseconds",
  "MistralAIAPIKeyLabel": "MistralAI API Key",
  "NijiVoiceActorId": "Actor ID",
  "NijiVoiceApiKey": "NijiVoice API Key",
  "NijiVoiceEmotionalLevel": "Emotional Level",
  "NijiVoiceInfo": "NijiVoice API is used. It supports only Japanese. API key can be obtained from the URL below.",
  "NijiVoiceSoundDuration": "Sound Duration",
  "NijiVoiceSpeed": "Speech Speed",
  "NoSpeechTimeout": "No Speech Timeout",
  "NoSpeechTimeoutInfo": "Set the time to automatically end input when silence continues during voice input.\nSetting to 0 seconds disables automatic submission by silence detection.",
  "NotConnectedToExternalAssistant": "Not connected to an external assistant.",
  "OpenAIAPIKeyLabel": "OpenAI API Key",
  "OpenAITTSInfo": "Using OpenAI. It supports multiple languages. If you select OpenAI as the AI service, you do not need to set the API key below.",
  "OpenAITTSModel": "Model",
  "OpenAITTSSpeed": "Speed",
  "OpenAITTSVoice": "Voice type",
  "OpenSendMessagePage": "Open Send Message Page",
  "OpenVRM": "Open VRM",
  "OtherSettings": "Others",
  "PdfConvertButton": "Convert PDF to slides",
  "PdfConvertDescription": "Convert PDF to slide mode data. Available only when the selected AI service is OpenAI, Anthropic Claude, or Google Gemini.",
  "PdfConvertError": "Conversion failed",
  "PdfConvertFileUpload": "Select PDF file",
  "PdfConvertFolderName": "Save folder name",
  "PdfConvertLabel": "PDF Slide Conversion",
  "PdfConvertLoading": "Converting...",
  "PdfConvertModelSelect": "Select model",
  "PdfConvertSubmitError": "Please make sure the PDF file, folder name, and API key are set.",
  "PdfConvertSuccess": "Conversion completed",
  "PerplexityAPIKeyLabel": "Perplexity API Key",
  "PresetQuestions": "Preset Questions",
  "PresetQuestionsInfo": "You can create and register multiple question patterns in advance. Registered questions will be displayed as buttons on the user UI, and clicking them will set them in the chat input field.",
  "RealtimeAPIMode": "Realtime API Mode",
  "RealtimeAPIModeContentType": "Send Type",
  "RealtimeAPIModeVoice": "Voice Type",
  "RepositoryURL": "Repository URL:",
  "SearchGrounding": "Use Search Grounding",
  "SearchGroundingDescription": "When using the multi-modal feature, the search function is automatically disabled.",
  "Select": "Select",
  "SelectAIService": "Select AI Service",
  "SelectModel": "Select Model",
  "SelectedSlideDocs": "Selected Slide Documents",
  "SendMessage": {
    "aiGenerateDescription": "The AI generates a response from the message sent and then speaks it. If multiple messages are sent, they are processed in order. The AI model and voice model are the ones selected in the AITuberKit settings. The system prompt can be selected to use the AITuberKit system prompt or a custom system prompt. If you want to load the past conversation history, include the string [conversation_history] in the system prompt or user message.",
    "aiGenerateTitle": "Generate AI response and then speak",
    "directSendDescription": "You can send the message directly to the AI character. If multiple messages are sent, they are processed in order. The voice model is the one selected in the AITuberKit settings.",
    "directSendTitle": "Directly speak to AI character",
    "title": "AITuberKit External Adapter",
    "useCurrentSystemPrompt": "Use AITuberKit system prompt",
    "userInputDescription": "The message sent is processed the same as when input from the AITuberKit input form. If multiple messages are sent, they are processed in order. The AI model and voice model are the ones selected in the AITuberKit settings. The system prompt and conversation history are the values set in AITuberKit.",
    "userInputTitle": "Send user input"
  },
  "ShowAssistantText": "Show answer box",
  "ShowCharacterName": "Show character name in the answer box",
  "ShowCharacterPresetMenu": "Show Character Preset Menu Button",
  "ShowControlPanel": "Show settings button",
  "ShowControlPanelInfo": "You can display the settings screen by pressing Cmd + . (Mac) / Ctrl + . (Windows).\nIf you are using a smartphone, you can also long-press the top left of the screen (for about 1 second).",
  "ShowSilenceProgressBar": "Show Silence Detection Progress Bar",
  "SlideMode": "Slide Mode",
  "SlideModeDescription": "This is a mode where AI automatically presents slides. It is only available when the selected AI service is OpenAI, Anthropic Claude, or Google Gemini.",
  "SlideSettings": "Slide Settings",
  "SourceCodeDescription1": "The source code for this app is publicly available on GitHub. Feel free to modify and adapt it as you like.",
  "SourceCodeDescription2": "For commercial use, please refer to the README of the same repository.",
  "SpeakerSelection": "Speaker Selection",
  "SpeechInputSettings": "Speech Input Settings",
  "SpeechRecognitionMode": "Speech Recognition Mode",
  "SpeechRecognitionModeDisabledInfo": "If audio mode is enabled, only browser speech recognition is available.\nAlso, in real-time API mode, only browser speech recognition is available, and the speech recognition timeout feature will be disabled.",
  "SpeechRecognitionModeInfo": "You can select the speech recognition mode.\n\"Browser Standard\" uses the built-in speech recognition of the browser. \"OpenAI TTS\" uses OpenAI's Text to Speech API.\nGenerally, \"Browser Standard\" is recommended as it has higher accuracy and faster recognition speed. However, if you are using a browser that does not support the WebSpeech API, such as Firefox, please select \"OpenAI TTS.\"",
  "StatusOff": "Status: OFF",
  "StatusOn": "Status: ON",
  "StyleBeatVITS2ApiKey": "API Key",
  "StyleBeatVITS2Length": "Speech Rate",
  "StyleBeatVITS2ModelID": "Model ID",
  "StyleBeatVITS2SdpRatio": "SDP/DP Mixing Ratio",
  "StyleBeatVITS2ServerURL": "Server URL",
  "StyleBeatVITS2Style": "Style",
  "StyleBertVITS2Info": "Using Style-Bert-VITS2. It supports only Japanese, English, and Chinese. If using a local API, you need to download and launch the app that suits your environment from the site below. Please also set up an API key if necessary.",
  "SyntheticVoiceEngineChoice": "Choose Synthetic Voice Engine",
  "TechnologyIntroduction": "Technology Introduction",
  "TechnologyIntroductionDescription1": "This app was created by modifying pixiv's <b>ChatVRM</b>. The original source code can be found",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "For displaying and manipulating 3D models,",
  "TechnologyIntroductionDescription4": "is used. For generating conversation text, various LLMs such as",
  "TechnologyIntroductionDescription5": "are used. For speech synthesis, various TTS engines like",
  "TechnologyIntroductionDescription6": "are utilized. For more details, please check out this",
  "TechnologyIntroductionDescription7": ".",
  "TechnologyIntroductionLink1": "here",
  "TechnologyIntroductionLink2": "explanatory article",
  "Temperature": "Temperature",
  "TestSelectedVoice": "Play",
  "TestVoice": "Test Voice",
  "TestVoiceSettings": "Voice Test",
  "Toasts": {
    "FirefoxNotSupported": "This feature is not supported on Firefox",
    "FunctionExecuting": "Executing {{funcName}}",
    "FunctionExecutionFailed": "Execution of {{funcName}} failed",
    "NoSpeechDetected": "No speech detected",
    "PresetSwitching": "Switched to {{presetName}}.",
    "SpeechRecognitionError": "Speech recognition error occurred",
    "WebSocketConnectionAttempt": "Attempting WebSocket connection...",
    "WebSocketConnectionClosed": "WebSocket connection closed",
    "WebSocketConnectionError": "Error occurred in WebSocket connection",
    "WebSocketConnectionSuccess": "WebSocket connection successful",
    "WhisperError": "An error occurred in speech recognition by Whisper"
  },
  "UpdateRealtimeAPISettings": "Update Realtime API Settings",
  "UpdateRealtimeAPISettingsInfo": "When updating the API key, Azure Endpoint, voice type, model, or system prompt, please press the update button to start a new WebSocket session.",
  "UpdateSpeakerList": "Update Speaker List",
  "UploadBackground": "Upload background image",
  "UseVideoAsBackground": "Use shared screen or webcam as background",
  "UsingAivisSpeech": "AivisSpeech",
  "UsingAzureTTS": "Using Azure OpenAI",
  "UsingElevenLabs": "ElevenLabs",
  "UsingGSVITTS": "GSVI TTS",
  "UsingGoogleTTS": "Use Google Text-to-Speech",
  "UsingKoeiromap": "Koeiromap",
  "UsingNijiVoice": "NijiVoice",
  "UsingOpenAITTS": "Using OpenAI",
  "UsingStyleBertVITS2": "Style-Bert-VITS2",
  "UsingVoiceVox": "VOICEVOX",
  "VoiceAdjustment": "Voice Adjustment",
  "VoiceEngineInstruction": "Select the synthetic voice engine you want to use.",
  "VoiceSettings": "Speech Synthesis Settings",
  "VoiceVoxInfo": "Using VOICEVOX. It only supports Japanese. It uses a local API, you need to download and launch the app that suits your environment from the site below.",
  "VoicevoxIntonation": "Intonation",
  "VoicevoxPitch": "Pitch",
  "VoicevoxServerUrl": "VOICEVOX Server URL",
  "VoicevoxSpeed": "Speed",
  "WhisperAPIKeyInfo": "An OpenAI API key is required for Whisper mode. Please set the OpenAI API key in the AI settings.",
  "WhisperSpeechRecognition": "Use OpenAI TTS Speech Recognition",
  "WhisperTranscriptionModel": "Transcription Model",
  "WhisperTranscriptionModelInfo": "You can select the model to be used for speech recognition. Higher performance models can recognize with higher accuracy, but may incur higher API costs.",
  "YoutubeAPIKey": "YouTube API Key",
  "YoutubeInfo": "The first character of the comment is '#', it is ignored.",
  "YoutubeLiveID": "YouTube Live ID",
  "YoutubeMode": "YouTube Mode",
  "YoutubeSettings": "YouTube Settings",
  "characterpresetInfo": "Selecting a preset changes the character prompt.\nShortcuts are available by pressing Cmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows).\nHolding down the Shift key and selecting a preset will save the current character prompt to the preset."
}
