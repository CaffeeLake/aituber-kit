{
  "AISettings": "KI-Einstellungen",
  "APIKeyInstruction": "API-Schlüssel können über den folgenden Link bezogen werden. Bitte geben Sie den erhaltenen API-Schlüssel in das Formular ein.",
  "APIKeyNotEntered": "API-Schlüssel wurde nicht eingegeben.",
  "AboutThisApplication": "Über diese Anwendung",
  "AboutThisApplicationDescription": "Genießen Sie Gespräche mit 3D-Charakteren nur mit einem Webbrowser, unter Verwendung von Mikrofon, Texteingabe und Sprachsynthese. Sie können den Charakter (VRM) ändern, Persönlichkeitseinstellungen vornehmen und die Stimme anpassen.<br />Einstellungen können über die Menütaste in der oberen linken Ecke geändert werden.",
  "AboutThisApplicationDescription2": "Mit AITuberKit können Sie Gespräche mit KI-Charakteren direkt in Ihrem Webbrowser genießen. Informationen zur Änderung des Charakters, Persönlichkeitseinstellungen und Stimmanpassung finden Sie in den jeweiligen Einstellungsbereichen.",
  "AivisSpeechInfo": "Verwendet AivisSpeech. Unterstützt nur Japanisch. Da eine lokale API verwendet wird, müssen Sie die passende Anwendung von der folgenden Website herunterladen und starten.",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechPitch": "Tonhöhe",
  "AivisSpeechServerUrl": "AivisSpeech Server-URL",
  "AivisSpeechSpeaker": "Sprecher",
  "AivisSpeechSpeed": "Sprechgeschwindigkeit",
  "AnswerGenerating": "Antwort wird generiert",
  "AnthropicAPIKeyLabel": "Anthropic API-Schlüssel",
  "AudioMode": "Audiomodus",
  "AuthFileInstruction": "Ein API-Schlüssel oder eine JSON-Authentifizierungsdatei ist erforderlich. Beziehen Sie diese von unten und platzieren Sie die JSON-Datei als 'credentials.json' im Root-Verzeichnis des Repositories.",
  "AzureAPIKeyLabel": "Azure OpenAI API-Schlüssel",
  "AzureAPIURL": "Azure OpenAI API URL",
  "AzureEndpoint": "Azure-Endpunkt",
  "AzureTTSInfo": "Verwendet Azure OpenAI. Unterstützt mehrere Sprachen.",
  "BackgroundImage": "Hintergrundbild",
  "BackgroundSettings": "Hintergrundeinstellungen",
  "BackgroundSettingsDescription": "Sie können ein Hintergrundbild für die Anwendung hochladen und auswählen.",
  "BasedSettings": "Grundeinstellungen",
  "BrowserSpeechRecognition": "Browser-Standardspracherkennung verwenden",
  "CannotUseParameters": "Bei aktiviertem Echtzeit-API-Modus oder Audiomodus können Temperature- und Max Tokens-Parameter nicht festgelegt werden.",
  "CannotUseVoice": "Bei aktiviertem Echtzeit-API-Modus oder Audiomodus\nsind Sprachsynthese-Einstellungen nicht erforderlich.",
  "ChangeBackgroundImage": "Hintergrundbild ändern",
  "CharacterModelInfo": "Bei einigen Modellen kann das initiale Laden längere Zeit in Anspruch nehmen.",
  "CharacterModelLabel": "Charaktermodell",
  "CharacterName": "Charaktername",
  "CharacterSettings": "Charaktereinstellungen",
  "CharacterSettingsInfo": "Dieser Wert wird als Systemprompt festgelegt.\nBasierend auf dem initialen Prompt können Sie Gefühlstags angeben, um die Ausdrücke und Bewegungen des Charakters zu steuern. Beispiel: [neutral]Guten Morgen![happy]Schön, Sie zu sehen!",
  "CharacterSettingsPrompt": "Charakterprompt",
  "Characterpreset1": "Voreinstellung 1",
  "Characterpreset2": "Voreinstellung 2",
  "Characterpreset3": "Voreinstellung 3",
  "Characterpreset4": "Voreinstellung 4",
  "Characterpreset5": "Voreinstellung 5",
  "CharacterpresetInfo": "Durch Auswahl einer Voreinstellung wird der Charakterprompt geändert.\nKurzbefehle sind möglich mit Cmd + Shift + 1-5 (Mac) / Ctrl + Shift + 1-5 (Windows).",
  "ChatLog": "Gesprächsprotokoll",
  "ClientID": "Client-ID",
  "Close": "Schließen",
  "CohereAPIKeyLabel": "Cohere API-Schlüssel",
  "Contact": "Kontakt",
  "ContactDescription": "Für Anfragen zu dieser App kontaktieren Sie uns bitte unter folgender E-Mail-Adresse oder Twitter-Account.",
  "ContinuousMic": "Dauerhafte Mikrofonaufnahme",
  "ContinuousMicActive": "Dauerhafte Mikrofonaufnahme aktiv",
  "ContinuousMicInfo": "Die Mikrofonaufnahme wird automatisch nach Beendigung der KI-Sprachausgabe neu gestartet. Nach Ablauf der konfigurierten Stillezeit wird die Aufnahme automatisch gesendet.\nWenn keine Spracherkennung innerhalb der konfigurierten Zeit erfolgt, wird die dauerhafte Mikrofonaufnahme automatisch deaktiviert. Wenn Sie sie immer aktiv halten möchten, setzen Sie das Spracherkennungs-Timeout auf 0 Sekunden.",
  "ContinuousMicModeOff": "Dauerhafte Mikrofonaufnahme-Modus ist AUS",
  "ContinuousMicModeOn": "Dauerhafte Mikrofonaufnahme-Modus ist EIN",
  "ConversationContinuityMode": "Gesprächskontinuitätsmodus (Beta)",
  "ConversationContinuityModeInfo": "In diesem Modus versucht die KI, das Gespräch selbstständig fortzusetzen, wenn keine Kommentare vorhanden sind. Derzeit nur für OpenAI, Anthropic Claude und Google Gemini verfügbar.",
  "ConversationContinuityModeInfo2": "Da bei einer Antwort mehrere LLM-Aufrufe erfolgen, können sich die API-Nutzungsgebühren erhöhen. Bitte beachten Sie dies.",
  "ConversationContinuityModeInfo3": "Relativ stabile Funktion mit gpt-4o, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet.",
  "ConversationHistory": "Gesprächsverlauf",
  "ConversationHistoryInfo": "Die letzten {{count}} Gesprächssätze werden als Gedächtnis gespeichert.",
  "ConversationHistoryReset": "Gesprächsverlauf zurücksetzen",
  "Creator": "Erstellerinformation",
  "CreatorDescription": "Ersteller: Nike",
  "CustomAPIBody": "Benutzerdefinierter Body",
  "CustomAPIBodyInfo": "Geben Sie die Body-Informationen im JSON-Format ein, die in API-Anfragen enthalten sein sollen. Nachrichten werden automatisch inkludiert.",
  "CustomAPIDescription": "Hinweis: Nachrichten werden automatisch in den Anfrage-Body aufgenommen. Im Streaming-Modus muss der Server text/event-stream zurückgeben.",
  "CustomAPIEndpoint": "Benutzerdefinierter API-Endpunkt",
  "CustomAPIEndpointInfo": "Geben Sie die URL des API-Endpunkts ein, an den POST-Anfragen gesendet werden sollen.",
  "CustomAPIHeaders": "Benutzerdefinierte Header",
  "CustomAPIHeadersInfo": "Geben Sie die Header-Informationen im JSON-Format ein, die in API-Anfragen enthalten sein sollen.",
  "CustomAPIStream": "Streaming-Modus",
  "CustomAPIStreamForced": "Der Streaming-Modus ist derzeit immer aktiviert.",
  "CustomVoiceTextPlaceholder": "Geben Sie Text zum Testen ein",
  "DeepSeekAPIKeyLabel": "DeepSeek API-Schlüssel",
  "DefaultBackground": "Standard-Hintergrund",
  "Description": "Über die App",
  "DifyAPIKeyLabel": "Dify API-Schlüssel",
  "DifyInfo": "Bei Dify werden nur Chatbot- oder Agententypen unterstützt.",
  "DifyInfo2": "Die Länge des Gesprächsverlaufs hängt von den Einstellungen des Dify-Chatbots ab.",
  "DifyInfo3": "Beispiel: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Bei Verwendung von Dify wird dieser Systemprompt nicht verwendet. Bitte konfigurieren Sie ihn im Dify-Chatbot.",
  "Documentation": "Dokumentation",
  "DocumentationDescription": "Detaillierte Anleitungen und Tutorials für AITuberKit finden Sie unter folgender URL.",
  "DontShowIntroductionNextTime": "Diesen Dialog beim nächsten Mal nicht anzeigen",
  "DragToReorder": "Ziehen, um die Reihenfolge zu ändern",
  "EditSlideScripts": "セリフ編集",
  "ElevenLabsApiKey": "ElevenLabs API-Schlüssel",
  "ElevenLabsInfo": "Verwendet die ElevenLabs API. Unterstützt mehrere Sprachen. Bitte beziehen Sie den API-Schlüssel von der folgenden URL.",
  "ElevenLabsVoiceId": "ElevenLabs Voice-ID",
  "ElevenLabsVoiceIdInfo": "Bitte wählen Sie die Voice-ID von der folgenden URL aus.",
  "EnglishToJapanese": "Englische Wörter auf Japanisch vorlesen",
  "EnterPresetQuestion": "Bitte geben Sie eine Frage ein",
  "EnterURL": "URL eingeben",
  "EnterYourQuestion": "Geben Sie Ihre Frage ein",
  "Errors": {
    "AIAPIError": "Fehler bei der Ausführung der KI-API",
    "AIInvalidProperty": "Ungültiger KI-Dienstkonfigurationswert",
    "CustomAPIError": "Fehler in der benutzerdefinierten API",
    "EmptyAPIKey": "API-Schlüssel ist nicht konfiguriert",
    "EmptyLocalLLMURL": "URL des lokalen LLM ist nicht konfiguriert",
    "InvalidAIService": "Der ausgewählte KI-Dienst ist ungültig",
    "InvalidJSON": "Ungültiges JSON-Format",
    "LocalLLMAPIError": "Fehler in der lokalen LLM-API",
    "LocalLLMConnectionError": "Verbindung zum lokalen LLM-Server nicht möglich",
    "LocalLLMError": "Fehler im lokalen LLM",
    "LocalLLMNotFound": "Endpunkt des lokalen LLM nicht gefunden",
    "LocalLLMStreamError": "Fehler bei der Stream-Verarbeitung des lokalen LLM",
    "MethodNotAllowed": "Anfrage ist nicht angemessen",
    "TTSServiceError": "Fehler im {{serviceName}} TTS-Dienst: {{message}}",
    "UnexpectedError": "Ein unerwarteter Fehler ist aufgetreten"
  },
  "ExternalLinkageMode": "Externe Verbindungsmodus (Beta)",
  "FireworksAPIKeyLabel": "Fireworks API-Schlüssel",
  "GSVITTSBatchSize": "GSVI TTS-Batchgröße (1 ~ 100, höhere Werte beschleunigen die Inferenz, können aber bei zu großen Werten den Speicher erschöpfen)",
  "GSVITTSInfo": "GSVI TTS-Einstellungen",
  "GSVITTSModelID": "GSVI TTS-Modell-ID",
  "GSVITTSServerUrl": "GSVI TTS-Server-URL",
  "GSVITTSSpeechRate": "Sprechgeschwindigkeit (0.5 ~ 2.0, höhere Werte sind schneller)",
  "GoogleAPIKeyLabel": "Google Gemini API-Schlüssel",
  "GoogleTTSInfo": "Verwendet Google Cloud Text-to-Speech. Unterstützt mehrere Sprachen.",
  "GroqAPIKeyLabel": "Groq API-Schlüssel",
  "GroqInfo": "Die Groq API wird direkt vom Browser aus aufgerufen.",
  "IncludeSystemMessages": "Systemnachrichten einschließen",
  "IncludeTimestampInUserMessage": "Zeitstempel in Benutzernachrichten einbeziehen",
  "IncludeTimestampInUserMessageInfo": "Durch Einbeziehung eines Zeitstempels in Benutzernachrichten kann die KI die Zeit bei der Generierung von Antworten berücksichtigen.\nBitte fügen Sie den folgenden Satz in Ihren Systemprompt ein:\n\n\"Benutzeranfragen können mit einem [timestamp] versehen sein. Dieser gibt die UTC-Zeit zum Zeitpunkt der Anfrage an, bitte berücksichtigen Sie diese Zeit bei der Generierung Ihrer Antwort.\"",
  "InitialSpeechTimeout": "Spracherkennungs-Timeout",
  "InitialSpeechTimeoutInfo": "Legt fest, wie lange nach dem Start der Spracherkennung auf die erste Sprachaufnahme gewartet wird. Wird in dieser Zeit keine Sprache erkannt, stoppt die Spracherkennung automatisch.\nBei Einstellung auf 0 Sekunden ist die Wartezeit unbegrenzt.",
  "InputAudio": "Audio",
  "InputText": "Text",
  "KoeiromapInfo": "Verwendet die Koeiromap API von Koemotion. Unterstützt nur Japanisch. Für weitere Informationen siehe unten.",
  "Language": "Spracheinstellungen",
  "LanguageChoice": "Sprachauswahl",
  "LanguageModelURL": "Bitte wählen Sie das Sprachmodell über folgende URL aus.",
  "ListeningContinuously": "Warte auf Spracheingabe...",
  "Live2D": {
    "EmotionInfo": "Emotionen können durch Kommas getrennt mehrfach angegeben werden. Bei mehreren Angaben wird zufällig ausgewählt.\nDie Standardwerte sind für die von AITuberKit bereitgestellten Modelle optimiert. Bei eigenen Modellen passen Sie die Werte bitte entsprechend an.\nNach Abschluss des Gesprächs wird der \"normale\" Ausdruck angezeigt.",
    "Emotions": "Ausdruckseinstellungen",
    "FileInfo": "Platzieren Sie den Ordner mit Ihrem gewünschten Live2D-Modell im Verzeichnis public/live2d. In diesem Ordner muss sich direkt eine model3.json-Datei befinden.\nWenn die Auswahlmöglichkeit nicht angezeigt wird, laden Sie den Bildschirm neu oder überprüfen Sie, ob der Pfad korrekt ist.",
    "Info": "Sie können Emotionen und Bewegungen festlegen.\nJede Emotion wird über den Prompt gesteuert. Weitere Details finden Sie unter \"KI-Einstellungen => Charaktereinstellungen\".",
    "MotionGroups": "Bewegungsgruppen-Einstellungen",
    "MotionGroupsInfo": "Bewegungen werden zufällig aus der gewählten Gruppe ausgewählt.\nPassen Sie diese wie bei den Ausdruckseinstellungen an Ihr eigenes Modell an.\n\"Im Leerlauf\" ist die Bewegung, die nach Abschluss des Gesprächs angezeigt wird.",
    "SelectMotionGroup": "Bewegungsgruppe auswählen",
    "angryEmotions": "Wütend",
    "angryMotionGroup": "Wütend",
    "happyEmotions": "Fröhlich",
    "happyMotionGroup": "Fröhlich",
    "idleMotionGroup": "Im Leerlauf",
    "neutralEmotions": "Normal",
    "neutralMotionGroup": "Normal",
    "relaxedEmotions": "Entspannt",
    "relaxedMotionGroup": "Entspannt",
    "sadEmotions": "Traurig",
    "sadMotionGroup": "Traurig",
    "surprisedEmotions": "Überrascht",
    "surprisedMotionGroup": "Überrascht"
  },
  "LocalLLM": "Lokales LLM",
  "LocalLLMInfo": "Ein lokaler LLM-Server muss gestartet sein.",
  "LocalLLMInfo2": "Bitte geben Sie die URL des lokalen LLM (mit Portnummer) und den Modellnamen ein.",
  "LocalStorageReset": "Einstellungen zurücksetzen",
  "LocalStorageResetButton": "Einstellungen zurücksetzen",
  "LocalStorageResetInfo": "Bei konfigurierten Umgebungsvariablen haben diese Vorrang. Die Seite wird neu geladen.",
  "LogSettings": "Gesprächsverlauf",
  "MaxPastMessages": "Anzahl der gespeicherten vorherigen Nachrichten",
  "MaxTokens": "Maximale Token-Anzahl",
  "MaxTokensInfo": "Die maximale Token-Anzahl variiert je nach verwendetem KI-Modell. Bitte überprüfen Sie die Spezifikationen jedes Modells.",
  "MessageReceiver": "Externe Anweisungen akzeptieren",
  "MessageReceiverDescription": "Ermöglicht es, die Aussagen des KI-Charakters über eine API extern zu steuern.",
  "Milliseconds": "Millisekunden",
  "MistralAIAPIKeyLabel": "MistralAI API-Schlüssel",
  "NijiVoiceActorId": "Sprecher-ID",
  "NijiVoiceApiKey": "NijiVoice API-Schlüssel",
  "NijiVoiceEmotionalLevel": "Emotionslevel",
  "NijiVoiceInfo": "Verwendet die NijiVoice API. Unterstützt nur Japanisch. Bitte beziehen Sie den API-Schlüssel von der folgenden URL.",
  "NijiVoiceSoundDuration": "Sprachdauer",
  "NijiVoiceSpeed": "Sprechgeschwindigkeit",
  "NoSpeechTimeout": "Stilleerkennung-Timeout",
  "NoSpeechTimeoutInfo": "Legt fest, nach welcher Zeit ohne Spracheingabe die Eingabe automatisch beendet wird.\nBei Einstellung auf 0 Sekunden wird das automatische Senden bei Stille deaktiviert.",
  "NotConnectedToExternalAssistant": "Nicht mit externem Assistenten verbunden.",
  "OpenAIAPIKeyLabel": "OpenAI API-Schlüssel",
  "OpenAITTSInfo": "Verwendet OpenAI. Unterstützt mehrere Sprachen. Wenn Sie OpenAI als KI-Dienst ausgewählt haben, müssen Sie den API-Schlüssel unten nicht konfigurieren.",
  "OpenAITTSModel": "Modell",
  "OpenAITTSSpeed": "Sprechgeschwindigkeit",
  "OpenAITTSVoice": "Stimmtyp",
  "OpenSendMessagePage": "Nachrichtenseite öffnen",
  "OpenVRM": "VRM öffnen",
  "OtherSettings": "Sonstiges",
  "PdfConvertButton": "PDF in Präsentation konvertieren",
  "PdfConvertDescription": "Konvertiert PDFs in Daten für den Präsentationsmodus. Nur verfügbar, wenn OpenAI, Anthropic Claude oder Google Gemini als KI-Dienst ausgewählt ist.",
  "PdfConvertError": "Konvertierung fehlgeschlagen",
  "PdfConvertFileUpload": "PDF-Datei auswählen",
  "PdfConvertFolderName": "Speicherordnername",
  "PdfConvertLabel": "PDF-Folienkonvertierung",
  "PdfConvertLoading": "Konvertierung läuft...",
  "PdfConvertModelSelect": "Modell auswählen",
  "PdfConvertSubmitError": "Bitte überprüfen Sie, ob PDF-Datei, Ordnername und API-Schlüssel konfiguriert sind",
  "PdfConvertSuccess": "Konvertierung abgeschlossen",
  "PerplexityAPIKeyLabel": "Perplexity API-Schlüssel",
  "PleaseSelectSlide": "スライドを選択してください",
  "PresetQuestions": "Vordefinierte Fragen",
  "PresetQuestionsInfo": "Sie können mehrere Fragemuster im Voraus erstellen und registrieren. Registrierte Fragen werden als Schaltflächen in der Benutzeroberfläche angezeigt und können durch Anklicken in das Chat-Eingabefeld übernommen werden.",
  "RealtimeAPIMode": "Echtzeit-API-Modus",
  "RealtimeAPIModeContentType": "Sendetyp",
  "RealtimeAPIModeVoice": "Stimmtyp",
  "RepositoryURL": "Repository-URL:",
  "SearchGrounding": "Suchfunktion verwenden",
  "SearchGroundingDescription": "Bei Verwendung der Multimodal-Funktion wird die Suchfunktion automatisch deaktiviert.",
  "Select": "Bitte auswählen",
  "SelectAIService": "KI-Dienst auswählen",
  "SelectModel": "Modell auswählen",
  "SelectedSlideDocs": "Verwendete Präsentation",
  "SendMessage": {
    "aiGenerateDescription": "Die KI generiert eine Antwort auf die gesendete Nachricht und lässt den KI-Charakter diese sprechen. Bei mehreren Nachrichten werden diese der Reihe nach verarbeitet.\nEs werden das KI-Modell und das Sprachmodell aus den AITuberKit-Einstellungen verwendet.\nSie können entweder den Systemprompt von AITuberKit verwenden oder einen benutzerdefinierten Systemprompt angeben.\nUm den vorherigen Gesprächsverlauf zu laden, fügen Sie an einer beliebigen Stelle im Systemprompt oder in der Benutzernachricht die Zeichenfolge [conversation_history] ein.",
    "aiGenerateTitle": "KI-generierte Antwort sprechen lassen",
    "directSendDescription": "Die gesendete Nachricht wird direkt vom KI-Charakter gesprochen. Bei mehreren Nachrichten werden diese der Reihe nach verarbeitet.\nEs wird das in den AITuberKit-Einstellungen ausgewählte Sprachmodell verwendet.",
    "directSendTitle": "KI-Charakter direkt sprechen lassen",
    "title": "AITuberKit Externer Adapter",
    "useCurrentSystemPrompt": "AITuberKit-Systemprompt verwenden",
    "userInputDescription": "Die gesendete Nachricht wird wie eine über das AITuberKit-Eingabeformular eingegebene Nachricht behandelt. Bei mehreren Nachrichten werden diese der Reihe nach verarbeitet.\nEs werden das KI-Modell und das Sprachmodell aus den AITuberKit-Einstellungen verwendet.\nSystemprompt und Gesprächsverlauf aus AITuberKit werden verwendet.",
    "userInputTitle": "Benutzereingabe senden"
  },
  "ShowAssistantText": "Antwortbereich anzeigen",
  "ShowCharacterName": "Charakternamen im Antwortbereich anzeigen",
  "ShowCharacterPresetMenu": "Charaktervoreinstellungsmenüschaltfläche anzeigen",
  "ShowControlPanel": "Steuerungsleiste anzeigen",
  "ShowControlPanelInfo": "Der Einstellungsbildschirm kann mit Cmd + . (Mac) / Ctrl + . (Windows) angezeigt werden.\nBei Smartphone-Nutzung können Sie auch den oberen linken Bildschirmbereich lange drücken (ca. 1 Sekunde).",
  "ShowSilenceProgressBar": "Fortschrittsbalken für Stilleerkennung anzeigen",
  "SlideMode": "Präsentationsmodus",
  "SlideModeDescription": "In diesem Modus präsentiert die KI automatisch Folien. Nur verfügbar, wenn OpenAI, Anthropic Claude oder Google Gemini als KI-Dienst ausgewählt ist.",
  "SlideSettings": "Präsentationseinstellungen",
  "SourceCodeDescription1": "Der Quellcode dieser App ist auf GitHub verfügbar. Er kann frei geändert und modifiziert werden.",
  "SourceCodeDescription2": "Informationen zur kommerziellen Nutzung finden Sie in der README des Repositories.",
  "SpeakerSelection": "Stimmtyp auswählen",
  "SpeechInputSettings": "Spracheingabe-Einstellungen",
  "SpeechRecognitionMode": "Spracherkennungsmodus",
  "SpeechRecognitionModeDisabledInfo": "Im Audiomodus ist nur die Browser-Spracherkennung verfügbar.\nIm Echtzeit-API-Modus ist ebenfalls nur die Browser-Spracherkennung verfügbar, und die Spracherkennungs-Timeout-Funktion ist deaktiviert.",
  "SpeechRecognitionModeInfo": "Wählen Sie den Spracherkennungsmodus aus.\n\"Browser-Standard\" verwendet die integrierte Spracherkennung des Browsers. \"OpenAI TTS\" verwendet die Text-to-Speech-API von OpenAI.\nIn der Regel wird \"Browser-Standard\" empfohlen, da er genauer und schneller ist. Bei Verwendung von Browsern wie Firefox, die die WebSpeech-API nicht unterstützen, wählen Sie bitte \"OpenAI TTS\".",
  "StatusOff": "Status: AUS",
  "StatusOn": "Status: AN",
  "StyleBeatVITS2ApiKey": "API-Schlüssel",
  "StyleBeatVITS2Length": "Sprechgeschwindigkeit",
  "StyleBeatVITS2ModelID": "Modell-ID",
  "StyleBeatVITS2SdpRatio": "SDP/DP-Mischverhältnis",
  "StyleBeatVITS2ServerURL": "Server-URL",
  "StyleBeatVITS2Style": "Stil",
  "StyleBertVITS2Info": "Verwendet Style-Bert-VITS2. Unterstützt nur Japanisch, Englisch und Chinesisch. Bei Verwendung einer lokalen API müssen Sie die passende Anwendung von der folgenden Website herunterladen und starten. Bei Bedarf konfigurieren Sie bitte auch den API-Schlüssel.",
  "SyntheticVoiceEngineChoice": "Sprachsynthese-Engine auswählen",
  "TechnologyIntroduction": "Technologievorstellung",
  "TechnologyIntroductionDescription1": "Diese App wurde durch Modifikation von pixiv's <b>ChatVRM</b> erstellt. Der ursprüngliche Quellcode ist",
  "TechnologyIntroductionDescription2": "zu finden.",
  "TechnologyIntroductionDescription3": "Für die Anzeige und Steuerung von 3D-Modellen wird",
  "TechnologyIntroductionDescription4": "verwendet, für die Generierung von Gesprächstexten",
  "TechnologyIntroductionDescription5": "und andere LLMs, für die Sprachsynthese",
  "TechnologyIntroductionDescription6": "und andere TTS-Systeme. Weitere Details finden Sie in diesem",
  "TechnologyIntroductionDescription7": ".",
  "TechnologyIntroductionLink1": "hier",
  "TechnologyIntroductionLink2": "Erklärungsartikel",
  "Temperature": "Temperature",
  "TestSelectedVoice": "Abspielen",
  "TestVoice": "Stimme testen",
  "TestVoiceSettings": "Stimmtest",
  "Toasts": {
    "FirefoxNotSupported": "Firefoxではこの機能はサポートされていません",
    "FunctionExecuting": "{{funcName}}を実行しています",
    "FunctionExecutionFailed": "{{funcName}}の実行に失敗しました",
    "NoSpeechDetected": "音声が検出されませんでした。",
    "PresetSwitching": "{{presetName}}に切り替わりました。",
    "SpeechRecognitionError": "音声認識エラーが発生しました",
    "UsingTool": "{{toolName}}を使用中",
    "WebSocketConnectionAttempt": "WebSocket接続を試みています...",
    "WebSocketConnectionClosed": "WebSocket接続が閉じられました",
    "WebSocketConnectionError": "WebSocket接続にエラーが発生しました",
    "WebSocketConnectionSuccess": "WebSocket接続に成功しました",
    "WhisperError": "Whisperによる音声認識でエラーが発生しました"
  },
  "UpdateRealtimeAPISettings": "Echtzeit-API-Einstellungen aktualisieren",
  "UpdateRealtimeAPISettingsInfo": "Drücken Sie die Aktualisierungstaste, um eine neue WebSocket-Sitzung zu starten, wenn Sie den API-Schlüssel, Azure-Endpunkt, Stimmtyp, Modell oder Systemprompt aktualisiert haben.",
  "UpdateSpeakerList": "Sprecherliste aktualisieren",
  "UploadBackground": "Hintergrundbild hochladen",
  "UseVideoAsBackground": "Geteilten Bildschirm oder Webcam als Hintergrund verwenden",
  "UsingAivisSpeech": "AivisSpeech verwenden",
  "UsingAzureTTS": "Azure OpenAI verwenden",
  "UsingElevenLabs": "ElevenLabs verwenden",
  "UsingGSVITTS": "GSVI TTS verwenden",
  "UsingGoogleTTS": "Google Text-to-Speech verwenden",
  "UsingKoeiromap": "Koeiromap verwenden",
  "UsingNijiVoice": "NijiVoice verwenden",
  "UsingOpenAITTS": "OpenAI verwenden",
  "UsingStyleBertVITS2": "Style-Bert-VITS2 verwenden",
  "UsingVoiceVox": "VOICEVOX verwenden",
  "VoiceAdjustment": "Stimmanpassung",
  "VoiceEngineInstruction": "Bitte wählen Sie die zu verwendende Sprachsynthese-Engine aus.",
  "VoiceSettings": "Sprachsynthese-Einstellungen",
  "VoiceVoxInfo": "Verwendet VOICEVOX. Unterstützt nur Japanisch. Da eine lokale API verwendet wird, müssen Sie die passende Anwendung von der folgenden Website herunterladen und starten.",
  "VoicevoxIntonation": "Intonation",
  "VoicevoxPitch": "Tonhöhe",
  "VoicevoxServerUrl": "VOICEVOX Server-URL",
  "VoicevoxSpeed": "Sprechgeschwindigkeit",
  "WhisperSpeechRecognition": "OpenAI TTS-Spracherkennung verwenden",
  "WhisperTranscriptionModel": "Transkriptionsmodell",
  "WhisperTranscriptionModelInfo": "Wählen Sie das für die Spracherkennung zu verwendende Modell aus. Leistungsfähigere Modelle bieten höhere Genauigkeit, können aber höhere API-Kosten verursachen.",
  "YoutubeAPIKey": "YouTube API-Schlüssel",
  "YoutubeInfo": "Kommentare, die mit \"#\" beginnen, werden ignoriert.",
  "YoutubeLiveID": "YouTube Live-ID",
  "YoutubeMode": "YouTube-Modus",
  "YoutubeSettings": "YouTube-Einstellungen"
}
