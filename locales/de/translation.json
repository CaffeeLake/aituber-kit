{
  "AISettings": "KI-Einstellungen",
  "APIKeyInstruction": "Sie können den API-Schlüssel unten erhalten. Geben Sie den erhaltenen API-Schlüssel in das Formular ein.",
  "APIKeyNotEntered": "API-Schlüssel nicht eingegeben.",
  "AboutThisApplication": "Über diese Anwendung",
  "AboutThisApplicationDescription": "Erleben Sie Gespräche mit 3D-Charakteren direkt im Browser durch Mikrofon oder Text und Sprachsynthese. Sie können den Charakter (VRM) wechseln, die Persönlichkeit einstellen und die Stimme anpassen.\nEinstellungen können über die Menü-Schaltfläche oben links geändert werden.",
  "AboutThisApplicationDescription2": "Wenn Sie den Charakter ändern möchten, sehen Sie bitte den Tab \"Charaktereinstellungen\".",
  "AivisSpeechInfo": "Verwendet AivisSpeech. Unterstützt nur Japanisch. Verwendet lokale API, Sie müssen die für Ihr System geeignete Anwendung herunterladen und ausführen.",
  "AivisSpeechIntonation": "Intonation",
  "AivisSpeechPitch": "Tonhöhe",
  "AivisSpeechServerUrl": "AivisSpeech Server-URL",
  "AivisSpeechSpeaker": "Sprecher",
  "AivisSpeechSpeed": "Geschwindigkeit",
  "AnswerGenerating": "Antwort wird generiert",
  "AnthropicAPIKeyLabel": "Anthropic API-Schlüssel",
  "AudioMode": "Audiomodus",
  "AuthFileInstruction": "API-Schlüssel oder Authentifizierungsdatei erforderlich. Von der URL unten erhalten und bei JSON-Datei im Stammverzeichnis platzieren.",
  "AzureAPIKeyLabel": "Azure OpenAI API-Schlüssel",
  "AzureAPIURL": "Azure OpenAI API-URL",
  "AzureEndpoint": "Azure-Endpunkt",
  "AzureTTSInfo": "Azure OpenAI verwenden. Unterstützt mehrere Sprachen.",
  "BackgroundImage": "Hintergrundbild",
  "BackgroundSettings": "Hintergrundeinstellungen",
  "BackgroundSettingsDescription": "Sie können ein Hintergrundbild für die Anwendung hochladen und auswählen.",
  "BasedSettings": "Grundeinstellungen",
  "BrowserSpeechRecognition": "Browser Standard Spracherkennung verwenden",
  "CannotUseParameters": "Wenn der Echtzeit-API-Modus oder der Audiomodus aktiviert ist, können die Parameter Temperature und Max Tokens nicht angegeben werden.",
  "CannotUseVoice": "Wenn der Echtzeit-API-Modus oder der Audiomodus aktiviert ist,\n sind die Einstellungen für synthetische Sprache nicht erforderlich.",
  "ChangeBackgroundImage": "Hintergrundbild ändern",
  "CharacterModelInfo": "Das Modell kann beim ersten Anzeigen längere Ladezeiten haben.",
  "CharacterModelLabel": "Charaktermodell",
  "CharacterName": "Charaktername",
  "CharacterSettings": "Charaktereinstellungen",
  "CharacterSettingsInfo": "Dieser Wert wird als Systemprompt festgelegt.\nBeziehen Sie sich auf den ursprünglichen Prompt und geben Sie Emotions-Tags an, um die Ausdrücke und Bewegungen des Charakters zu steuern. Beispiel: [neutral]Guten Morgen![happy]Auch heute wird ein geschäftiger Tag!",
  "CharacterSettingsPrompt": "Charakter-Prompt",
  "Characterpreset1": "Preset 1",
  "Characterpreset2": "Preset 2",
  "Characterpreset3": "Preset 3",
  "Characterpreset4": "Preset 4",
  "Characterpreset5": "Preset 5",
  "CharacterpresetInfo": "Wenn Sie ein Preset auswählen, ändert sich der Charakter-Prompt.\nCmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows) ermöglicht Shortcuts.",
  "ChatLog": "Chat-Protokoll",
  "ClientID": "Client-ID",
  "Close": "SCHLIEßEN",
  "CohereAPIKeyLabel": "Cohere API-Schlüssel",
  "Contact": "Kontakt",
  "ContactDescription": "Bei Fragen zu dieser Anwendung kontaktieren Sie uns bitte über die unten stehende E-Mail-Adresse oder Twitter-Account.",
  "ContinuousMic": "Ständige Mikrofoneingabe",
  "ContinuousMicActive": "Ständige Mikrofoneingabe aktiv",
  "ContinuousMicInfo": "Das Mikrofon wird automatisch wieder aktiviert, wenn die Äußerung der KI beendet ist. Nach Ablauf der festgelegten Stillezeit wird automatisch gesendet.\nWenn die festgelegte Zeit überschritten wird, ohne dass eine Spracherkennung erfolgt, wird die ständige Mikrofoneingabe automatisch deaktiviert. Wenn Sie möchten, dass sie immer aktiviert bleibt, stellen Sie das Spracherkennung-Timeout auf 0 Sekunden ein.",
  "ContinuousMicModeOff": "Der Modus für ständige Mikrofoneingabe ist deaktiviert",
  "ContinuousMicModeOn": "Der Modus für ständige Mikrofoneingabe ist aktiviert",
  "ConversationContinuityMode": "Gesprächskontinuitätsmodus (Beta)",
  "ConversationContinuityModeInfo": "Wenn keine Kommentare vorhanden sind, versucht die KI das Gespräch fortzusetzen. Derzeit nur von OpenAI, Anthropic Claude, Google Gemini unterstützt.",
  "ConversationContinuityModeInfo2": "Eine Antwort erfordert mehrere LLM-Aufrufe, daher kann die API-Nutzung steigen. Bitte beachten Sie dies.",
  "ConversationContinuityModeInfo3": "gpt-4, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet funktionieren relativ stabil.",
  "ConversationHistory": "Gesprächsverlauf",
  "ConversationHistoryInfo": "Die letzten {{count}} Konversationen werden als Gedächtnis gespeichert.",
  "ConversationHistoryReset": "Gesprächsverlauf zurücksetzen",
  "Creator": "Ersteller",
  "CreatorDescription": "Ersteller: Tegan",
  "CustomAPIBody": "Benutzerdefinierter Body",
  "CustomAPIBodyInfo": "Bitte geben Sie die Body-Informationen im JSON-Format ein, die in die API-Anfrage aufgenommen werden sollen. messages werden automatisch hinzugefügt.",
  "CustomAPIDescription": "Hinweis: Nachrichten werden automatisch in den Anfrage-Body aufgenommen. Im Streaming-Modus muss der Server text/event-stream zurückgeben.",
  "CustomAPIEndpoint": "Benutzerdefinierter API-Endpunkt",
  "CustomAPIEndpointInfo": "Bitte geben Sie die URL des API-Endpunkts ein, an den die POST-Anfrage gesendet werden soll.",
  "CustomAPIHeaders": "Benutzerdefinierte Header",
  "CustomAPIHeadersInfo": "Bitte geben Sie die Header-Informationen im JSON-Format ein, die in die API-Anfrage aufgenommen werden sollen.",
  "CustomAPIStream": "Streaming-Modus",
  "CustomAPIStreamForced": "Der Streaming-Modus ist derzeit immer aktiviert.",
  "DeepSeekAPIKeyLabel": "DeepSeek API-Schlüssel",
  "DefaultBackground": "Standardhintergrund",
  "Description": "Über die App",
  "DifyAPIKeyLabel": "Dify API-Schlüssel",
  "DifyInfo": "Dify unterstützt nur Chatbot- und Agent-Typen.",
  "DifyInfo2": "Die Länge des Gesprächsverlaufs hängt von den Dify-Spezifikationen ab.",
  "DifyInfo3": "Beispiel: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Wenn Sie Dify verwenden, wird der Systemprompt nicht verwendet. Bitte konfigurieren Sie den Dify-Chatbot.",
  "DocumentationDescription": "Detaillierte Anleitungen und Tutorials zur Verwendung von AITuberKit finden Sie unter der folgenden URL.",
  "DontShowIntroductionNextTime": "Diesen Dialog beim nächsten Mal nicht anzeigen",
  "DragToReorder": "Ziehen, um die Reihenfolge zu ändern",
  "ElevenLabsApiKey": "ElevenLabs API-Schlüssel",
  "ElevenLabsInfo": "Verwendet ElevenLabs API. Unterstützt mehrere Sprachen. API-Schlüssel kann von der URL unten erhalten werden.",
  "ElevenLabsVoiceId": "ElevenLabs Stimmen-ID",
  "ElevenLabsVoiceIdInfo": "Stimmen-ID kann von der URL unten ausgewählt werden.",
  "EnterPresetQuestion": "Bitte Frage eingeben",
  "EnterURL": "URL",
  "EnterYourQuestion": "Geben Sie hier Ihre Frage ein",
  "Errors": {
    "AIAPIError": "Fehler bei der Ausführung der KI-API",
    "AIInvalidProperty": "KI-Dienstkonfiguration ist nicht korrekt",
    "CustomAPIError": "Ein Fehler ist bei der benutzerdefinierten API aufgetreten",
    "EmptyAPIKey": "API-Schlüssel ist nicht konfiguriert",
    "EmptyLocalLLMURL": "Die URL des lokalen LLM ist nicht festgelegt",
    "InvalidAIService": "Ausgewählter KI-Dienst ist ungültig",
    "InvalidJSON": "Das JSON-Format ist ungültig",
    "LocalLLMAPIError": "Lokaler LLM-API-Fehler",
    "LocalLLMConnectionError": "Verbindungsfehler zum lokalen LLM-Server",
    "LocalLLMError": "Lokaler LLM-Fehler",
    "LocalLLMNotFound": "Lokaler LLM-Endpunkt nicht gefunden",
    "LocalLLMStreamError": "Lokaler LLM-Stream-Fehler",
    "MethodNotAllowed": "Anfrage ist nicht angemessen",
    "TTSServiceError": "Fehler im TTS-Dienst {{serviceName}}: {{message}}",
    "UnexpectedError": "Ein unerwarteter Fehler ist aufgetreten"
  },
  "ExternalLinkageMode": "Externer Verknüpfungsmodus (Beta-Version)",
  "FireworksAPIKeyLabel": "Fireworks API-Schlüssel",
  "GSVITTSBatchSize": "GSVI TTS Batch-Größe (1 ~ 100 Je höher der Wert, desto schneller die Inferenz, aber zu groß kann Speicher erschöpfen)",
  "GSVITTSInfo": "GSVI TTS-Einstellungen",
  "GSVITTSModelID": "GSVI TTS Modell-ID",
  "GSVITTSServerUrl": "GSVI TTS API-Endpunkt",
  "GSVITTSSpeechRate": "Sprechgeschwindigkeit (0.5 ~ 2.0 Je höher der Wert, desto schneller)",
  "GoogleAPIKeyLabel": "Google Gemini API-Schlüssel",
  "GoogleTTSInfo": "Verwendet Google Cloud Text-to-Speech. Unterstützt mehrere Sprachen.",
  "GroqAPIKeyLabel": "Groq API-Schlüssel",
  "GroqInfo": "Die Groq-API wird direkt vom Browser aus zugegriffen.",
  "IncludeTimestampInUserMessage": "Zeitstempel in Benutzernachrichten einschließen",
  "IncludeTimestampInUserMessageInfo": "Das Einschließen des Zeitstempels hilft der KI, Antworten unter Berücksichtigung der Sendezeit zu generieren.\nBitte fügen Sie die folgende Zeichenfolge in den Systemprompt ein:\n\n\"Die Benutzereingabe kann [timestamp] enthalten. Dies ist die UTC-Zeit zum Zeitpunkt der Anfrage, bitte generieren Sie Antworten mit dieser Information.\"",
  "InitialSpeechTimeout": "Spracherkennung Timeout",
  "InitialSpeechTimeoutInfo": "Stellen Sie die Wartezeit ein, bis die erste Äußerung nach Beginn der Spracherkennung erkannt wird. Wenn innerhalb dieser Zeit keine Äußerung erkannt wird, wird die Spracherkennung automatisch gestoppt.\nWenn Sie 0 Sekunden einstellen, wird die Wartezeit unbegrenzt.",
  "InputAudio": "Audio",
  "InputText": "Text",
  "KoeiromapInfo": "Verwendet die Koeiromap-API von Koemotion. Unterstützt nur Japanisch. Siehe Link unten für Details.",
  "Language": "Sprache",
  "LanguageChoice": "Sprachauswahl",
  "LanguageModelURL": "Sprachmodell von der URL unten auswählen.",
  "ListeningContinuously": "Warte auf Sprachinput...",
  "Live2D": {
    "EmotionInfo": "Emotionen können im Komma-getrennten Format angegeben werden. Wenn mehrere Emotionen angegeben werden, werden sie zufällig ausgewählt.\nDer Anfangswert ist für das von AITuberKit bereitgestellte Modell. Wenn Sie ein eigenes Modell verwenden, geben Sie den Wert entsprechend Ihrem Modell ein.\nNach Abschluss des Gesprächs wird die Emotion \"Neutral\" angezeigt.",
    "Emotions": "Emotionseinstellungen",
    "FileInfo": "Platzieren Sie das Live2D-Modell, das Sie verwenden möchten, im Ordner public/live2d. Die Datei model3.json muss im Stammverzeichnis dieses Ordners existieren.\nWenn es nicht in der Auswahl angezeigt wird, laden Sie den Bildschirm neu oder überprüfen Sie, ob der Ordnerpfad korrekt ist.",
    "Info": "Sie können Emotionen und Bewegungen angeben.\nJede Emotion wird durch den Prompt gesteuert. Weitere Details finden Sie unter \"KI-Einstellungen => Charaktereinstellungen\".",
    "MotionGroups": "Bewegungsgruppen-Einstellungen",
    "MotionGroupsInfo": "Bewegungsgruppen werden zufällig aus der ausgewählten Gruppe ausgewählt.\nWie bei den Emotionseinstellungen konfigurieren Sie es entsprechend Ihrem Modell.\n\"Idle\" ist die Bewegung, die nach Abschluss des Gesprächs angezeigt wird.",
    "SelectMotionGroup": "Bewegungsgruppe auswählen",
    "angryEmotions": "Wütend",
    "angryMotionGroup": "Wütend",
    "happyEmotions": "Glücklich",
    "happyMotionGroup": "Glücklich",
    "idleMotionGroup": "Inaktiv",
    "neutralEmotions": "Neutral",
    "neutralMotionGroup": "Neutral",
    "relaxedEmotions": "Entspannt",
    "relaxedMotionGroup": "Entspannt",
    "sadEmotions": "Traurig",
    "sadMotionGroup": "Traurig",
    "surprisedEmotions": "Überraschung",
    "surprisedMotionGroup": "Überraschung"
  },
  "LocalLLM": "Lokales LLM",
  "LocalLLMInfo": "Der lokale LLM-Server muss ausgeführt werden. Die Konfiguration ist wie folgt.",
  "LocalLLMInfo2": "Geben Sie die lokale LLM-Server-URL (einschließlich Portnummer) und den Modellnamen ein.",
  "LocalStorageReset": "Einstellungen zurücksetzen",
  "LocalStorageResetButton": "Einstellungen zurücksetzen",
  "LocalStorageResetInfo": "Umgebungsvariablen haben Vorrang, wenn sie gesetzt sind. Die Seite wird neu geladen.",
  "LogSettings": "Gesprächsverlauf",
  "MaxPastMessages": "Anzahl der zu behaltenden vergangenen Nachrichten",
  "MaxTokens": "Maximale Tokenanzahl",
  "MaxTokensInfo": "Die maximale Tokenanzahl variiert je nach verwendetem KI-Modell. Bitte überprüfen Sie die Spezifikationen jedes Modells.",
  "MessageReceiver": "Anweisungen von außen empfangen",
  "MessageReceiverDescription": "Sie können die API verwenden, um KI-Charaktere von außen sprechen zu lassen.",
  "Milliseconds": "Millisekunden",
  "MistralAIAPIKeyLabel": "MistralAI API-Schlüssel",
  "NijiVoiceActorId": "Sprecher-ID",
  "NijiVoiceApiKey": "NijiVoice API-Schlüssel",
  "NijiVoiceEmotionalLevel": "Emotionslevel",
  "NijiVoiceInfo": "Verwendet NijiVoice-API. Unterstützt nur Japanisch. API-Schlüssel kann von der URL unten erhalten werden.",
  "NijiVoiceSoundDuration": "Tondauer",
  "NijiVoiceSpeed": "Sprechgeschwindigkeit",
  "NotConnectedToExternalAssistant": "Nicht mit externem Assistenten verbunden.",
  "OpenAIAPIKeyLabel": "OpenAI API-Schlüssel",
  "OpenAITTSInfo": "OpenAI verwenden. Unterstützt mehrere Sprachen. Wenn Sie OpenAI als KI-Dienst auswählen, müssen Sie den API-Schlüssel unten nicht konfigurieren.",
  "OpenAITTSModel": "Modell",
  "OpenAITTSSpeed": "Geschwindigkeit",
  "OpenAITTSVoice": "Stimmtyp",
  "OpenSendMessagePage": "Nachrichtenseite öffnen",
  "OpenVRM": "VRM öffnen",
  "OtherSettings": "Sonstiges",
  "PdfConvertButton": "PDF in Präsentation konvertieren",
  "PdfConvertDescription": "Konvertiert PDF in Präsentationsmodus-Daten. Nur verfügbar, wenn der ausgewählte KI-Dienst OpenAI, Anthropic Claude oder Google Gemini ist.",
  "PdfConvertError": "Konvertierung fehlgeschlagen",
  "PdfConvertFileUpload": "PDF-Datei auswählen",
  "PdfConvertFolderName": "Speicherordnername",
  "PdfConvertLabel": "PDF-Präsentationskonvertierung",
  "PdfConvertLoading": "Konvertierung läuft...",
  "PdfConvertModelSelect": "Modell auswählen",
  "PdfConvertSubmitError": "Bitte stellen Sie sicher, dass PDF-Datei, Ordnername und API-Schlüssel konfiguriert sind.",
  "PdfConvertSuccess": "Konvertierung abgeschlossen",
  "PerplexityAPIKeyLabel": "Perplexity API-Schlüssel",
  "PresetQuestions": "Voreingestellte Fragen",
  "PresetQuestionsInfo": "Sie können mehrere Frage-Muster im Voraus erstellen und registrieren. Die registrierten Fragen werden in Form von Schaltflächen in der Benutzeroberfläche angezeigt und beim Klicken in das Chat-Eingabefeld gesetzt.",
  "RealtimeAPIMode": "Echtzeit-API-Modus",
  "RealtimeAPIModeContentType": "Sendungstyp",
  "RealtimeAPIModeVoice": "Stimmtyp",
  "RepositoryURL": "Repository-URL:",
  "SearchGrounding": "Kontextuelle Suche verwenden",
  "SearchGroundingDescription": "Bei Verwendung der multimodalen Funktion wird die Suchfunktion automatisch deaktiviert.",
  "Select": "Auswählen",
  "SelectAIService": "KI-Dienst auswählen",
  "SelectModel": "Modell auswählen",
  "SelectedSlideDocs": "Ausgewählte Präsentationsdokumente",
  "SendMessage": {
    "aiGenerateDescription": "Die KI generiert eine Antwort aus der gesendeten Nachricht und spricht sie dann. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das KI-Modell und das Sprachmodell sind die in den AITuberKit-Einstellungen ausgewählten. Der Systemprompt kann ausgewählt werden, um entweder den AITuberKit-Systemprompt oder einen benutzerdefinierten Systemprompt zu verwenden. Wenn Sie den vorherigen Gesprächsverlauf laden möchten, fügen Sie die Zeichenfolge [conversation_history] in den Systemprompt oder die Benutzernachricht ein.",
    "aiGenerateTitle": "KI-Antwort generieren und dann sprechen",
    "directSendDescription": "Sie können die Nachricht direkt an den KI-Charakter senden. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das Sprachmodell ist das in den AITuberKit-Einstellungen ausgewählte.",
    "directSendTitle": "Direkt zum KI-Charakter sprechen",
    "title": "AITuberKit Externer Adapter",
    "useCurrentSystemPrompt": "AITuberKit-Systemprompt verwenden",
    "userInputDescription": "Die gesendete Nachricht wird wie bei der Eingabe über das AITuberKit-Eingabeformular verarbeitet. Wenn mehrere Nachrichten gesendet werden, werden sie der Reihe nach verarbeitet. Das KI-Modell und das Sprachmodell sind die in den AITuberKit-Einstellungen ausgewählten. Der Systemprompt und der Gesprächsverlauf sind die in AITuberKit konfigurierten Werte.",
    "userInputTitle": "Benutzereingabe senden"
  },
  "ShowAssistantText": "Antwortbox anzeigen",
  "ShowCharacterName": "Charakternamen in Antwortbox anzeigen",
  "ShowCharacterPresetMenu": "Charakter-Preset-Menü-Button anzeigen",
  "ShowControlPanel": "Einstellungsschaltfläche anzeigen",
  "ShowControlPanelInfo": "Das Einstellungsfenster kann mit Cmd + . (Mac) / Ctrl + . (Windows) angezeigt werden.\nWenn Sie ein Smartphone verwenden, können Sie auch die linke obere Ecke des Bildschirms etwa 1 Sekunde lang gedrückt halten.",
  "ShowSilenceProgressBar": "Fortschrittsanzeige für Stille anzeigen",
  "SlideMode": "Präsentationsmodus",
  "SlideModeDescription": "Dies ist ein Modus, in dem die KI automatisch Folien präsentiert. Nur verfügbar, wenn der ausgewählte KI-Dienst OpenAI, Anthropic Claude oder Google Gemini ist.",
  "SlideSettings": "Slide-Einstellungen",
  "SourceCodeDescription1": "Der Quellcode dieser Anwendung wird auf GitHub geteilt. Sie können ihn frei modifizieren.",
  "SourceCodeDescription2": "Für kommerzielle Nutzung beachten Sie bitte die README des Repositories.",
  "SpeakerSelection": "Sprecherauswahl",
  "SpeechInputSettings": "Sprachinput-Einstellungen",
  "SpeechRecognitionMode": "Spracherkennungsmodus",
  "SpeechRecognitionModeDisabledInfo": "Wenn der Audiomodus aktiviert ist, ist nur die Browser-Spracherkennung verfügbar.\nAußerdem ist im Echtzeit-API-Modus nur die Browser-Spracherkennung verfügbar, und die Funktion für das Spracherkennung-Timeout wird deaktiviert.",
  "SpeechRecognitionModeInfo": "Sie können den Spracherkennungsmodus auswählen.\n„Browser Standard“ verwendet die im Browser integrierte Spracherkennung. „OpenAI TTS“ verwendet die Text-to-Speech-API von OpenAI.\nIm Allgemeinen wird „Browser Standard“ empfohlen, da es eine höhere Genauigkeit und schnellere Erkennungszeiten bietet. Wenn Sie jedoch einen Browser verwenden, der die WebSpeech-API nicht unterstützt, wie Firefox, wählen Sie bitte „OpenAI TTS“.",
  "StatusOff": "Status: AUS",
  "StatusOn": "Status: EIN",
  "StyleBeatVITS2ApiKey": "API-Schlüssel",
  "StyleBeatVITS2Length": "Sprechgeschwindigkeit",
  "StyleBeatVITS2ModelID": "Modell-ID",
  "StyleBeatVITS2SdpRatio": "SDP/DP-Mischratio",
  "StyleBeatVITS2ServerURL": "Server-URL",
  "StyleBeatVITS2Style": "Stil",
  "StyleBertVITS2Info": "Verwendet Style-Bert-VITS2. Unterstützt nur Japanisch, Englisch und Chinesisch. Bei lokaler API müssen Sie die passende Anwendung herunterladen und ausführen. Bei Bedarf richten Sie bitte einen API-Schlüssel ein.",
  "SyntheticVoiceEngineChoice": "Sprachsynthese-Engine auswählen",
  "TechnologyIntroduction": "Technologie-Einführung",
  "TechnologyIntroductionDescription1": "Diese Anwendung basiert auf dem <b>ChatVRM</b>-Projekt von pixiv. Der ursprüngliche Quellcode ist",
  "TechnologyIntroductionDescription2": "verfügbar.",
  "TechnologyIntroductionDescription3": "Für die Anzeige und Manipulation von 3D-Modellen wird",
  "TechnologyIntroductionDescription4": "verwendet. Für die Gesprächsgenerierung werden verschiedene LLMs wie",
  "TechnologyIntroductionDescription5": "eingesetzt. Für die Sprachsynthese werden verschiedene TTS-Engines wie",
  "TechnologyIntroductionDescription6": "verwendet. Weitere Details finden Sie in diesem",
  "TechnologyIntroductionDescription7": ".",
  "TechnologyIntroductionLink1": "hier",
  "TechnologyIntroductionLink2": "Erklärungsartikel",
  "Temperature": "Temperatur",
  "TestVoice": "Sprachtest",
  "Toasts": {
    "FirefoxNotSupported": "Diese Funktion wird in Firefox nicht unterstützt",
    "FunctionExecuting": "Führe {{funcName}} aus",
    "FunctionExecutionFailed": "Ausführung von {{funcName}} fehlgeschlagen",
    "PresetSwitching": "Wechsel zu {{presetName}}.",
    "SpeechRecognitionError": "Ein Spracherkennungsfehler ist aufgetreten",
    "WebSocketConnectionAttempt": "WebSocket-Verbindungsversuch...",
    "WebSocketConnectionClosed": "WebSocket-Verbindung geschlossen",
    "WebSocketConnectionError": "WebSocket-Verbindungsfehler",
    "WebSocketConnectionSuccess": "WebSocket-Verbindung erfolgreich",
    "WhisperError": "Ein Fehler ist bei der Spracherkennung durch Whisper aufgetreten"
  },
  "UpdateRealtimeAPISettings": "Echtzeit-API-Einstellungen aktualisieren",
  "UpdateRealtimeAPISettingsInfo": "Wenn Sie API-Schlüssel, Azure-Endpunkt, Stimmtyp, Modell oder Systemprompt aktualisieren, drücken Sie die Aktualisierungsschaltfläche, um eine neue WebSocket-Sitzung zu starten.",
  "UpdateSpeakerList": "Sprecherliste aktualisieren",
  "UploadBackground": "Hintergrundbild hochladen",
  "UseVideoAsBackground": "Geteilten Bildschirm oder Webcam als Hintergrund verwenden",
  "UsingAivisSpeech": "AivisSpeech",
  "UsingAzureTTS": "Azure OpenAI verwenden",
  "UsingElevenLabs": "ElevenLabs",
  "UsingGSVITTS": "GSVI TTS",
  "UsingGoogleTTS": "Google Text-to-Speech verwenden",
  "UsingKoeiromap": "Koeiromap",
  "UsingNijiVoice": "NijiVoice",
  "UsingOpenAITTS": "OpenAI verwenden",
  "UsingStyleBertVITS2": "Style-Bert-VITS2",
  "UsingVoiceVox": "VOICEVOX",
  "VoiceAdjustment": "Sprachanpassung",
  "VoiceEngineInstruction": "Wählen Sie die Sprachsynthese-Engine aus, die Sie verwenden möchten.",
  "VoiceSettings": "Einstellungen für synthetische Sprache",
  "VoiceVoxInfo": "Verwendet VOICEVOX. Unterstützt nur Japanisch. Verwendet lokale API, Sie müssen die für Ihr System geeignete Anwendung herunterladen und ausführen.",
  "VoicevoxIntonation": "Intonation",
  "VoicevoxPitch": "Tonhöhe",
  "VoicevoxServerUrl": "VOICEVOX Server-URL",
  "VoicevoxSpeed": "Geschwindigkeit",
  "WhisperAPIKeyInfo": "Im Whisper-Modus ist ein OpenAI-API-Schlüssel erforderlich. Bitte geben Sie den OpenAI-API-Schlüssel in den KI-Einstellungen ein.",
  "WhisperSpeechRecognition": "OpenAI TTS Spracherkennung verwenden",
  "WhisperTranscriptionModel": "Transkriptionsmodell",
  "WhisperTranscriptionModelInfo": "Sie können das Modell auswählen, das für die Spracherkennung verwendet werden soll. Hochleistungsmodelle können genauer erkannt werden, können jedoch höhere API-Kosten verursachen.",
  "YoutubeAPIKey": "YouTube API-Schlüssel",
  "YoutubeInfo": "Das erste Zeichen des Kommentars ist '#' und wird ignoriert.",
  "YoutubeLiveID": "YouTube Live-ID",
  "YoutubeMode": "YouTube-Modus",
  "YoutubeSettings": "YouTube-Einstellungen",
  "characterpresetInfo": "Durch Auswahl einer Voreinstellung wird die Zeichenaufforderung geändert.\nCmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows) für Tastaturkürzel.\nWenn Sie eine Voreinstellung auswählen, während Sie die Umschalttaste gedrückt halten, wird die aktuelle Zeichenaufforderung in der Voreinstellung gespeichert."
}
