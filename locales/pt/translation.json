{
  "AISettings": "Configurações de IA",
  "APIKeyInstruction": "Você pode obter a chave API no link abaixo. Insira a chave API obtida no formulário.",
  "APIKeyNotEntered": "Chave API não inserida.",
  "AboutThisApplication": "Sobre este aplicativo",
  "AboutThisApplicationDescription": "Aproveite conversas com personagens 3D usando apenas o navegador da web, com entrada por microfone, texto e síntese de voz. Você pode modificar personagens (VRM), definir personalidades e ajustar a voz.<br />As configurações podem ser alteradas no botão de menu no canto superior esquerdo.",
  "AboutThisApplicationDescription2": "Com o AITuberKit, você pode desfrutar de conversas com personagens de IA usando apenas um navegador web. Verifique cada item de configuração para alterar o personagem, definir personalidades e ajustar a voz.",
  "AivisSpeechInfo": "Usando AivisSpeech. Suporta apenas japonês. Como usa a API local, você precisa baixar e iniciar o aplicativo adequado ao seu ambiente no site abaixo.",
  "AivisSpeechIntonation": "Entonação",
  "AivisSpeechPitch": "Tom",
  "AivisSpeechServerUrl": "URL do servidor AivisSpeech",
  "AivisSpeechSpeaker": "Locutor",
  "AivisSpeechSpeed": "Velocidade de fala",
  "AnswerGenerating": "Gerando resposta",
  "AnthropicAPIKeyLabel": "Chave API da Anthropic",
  "AudioMode": "Modo de áudio",
  "AuthFileInstruction": "É necessária uma chave API ou um arquivo JSON de autenticação. Obtenha-o abaixo e, caso seja um arquivo JSON, coloque-o na pasta raiz do repositório com o nome credentials.json.",
  "AzureAPIKeyLabel": "Chave API do Azure OpenAI",
  "AzureAPIURL": "URL da API do Azure OpenAI",
  "AzureEndpoint": "Azure Endpoint",
  "AzureTTSInfo": "Usando Azure OpenAI. Suporta vários idiomas.",
  "BackgroundImage": "Imagem de fundo",
  "BackgroundSettings": "Configurações de fundo",
  "BackgroundSettingsDescription": "Você pode carregar e selecionar imagens de fundo para o aplicativo.",
  "BasedSettings": "Configurações básicas",
  "BrowserSpeechRecognition": "Usar reconhecimento de voz padrão do navegador",
  "CannotUseParameters": "Quando o modo de API em tempo real ou o modo de áudio está ativado, os parâmetros de Temperatura e Número Máximo de Tokens não podem ser especificados.",
  "CannotUseVoice": "Quando o modo de API em tempo real ou o modo de áudio está ativado,\nas configurações de voz sintética não são necessárias.",
  "ChangeBackgroundImage": "Alterar imagem de fundo",
  "CharacterModelInfo": "Dependendo do modelo, o carregamento inicial pode levar algum tempo.",
  "CharacterModelLabel": "Modelo de personagem",
  "CharacterName": "Nome do personagem",
  "CharacterSettings": "Configurações de personagem",
  "CharacterSettingsInfo": "Este valor será definido como o prompt do sistema.\nReferindo-se ao prompt inicial, você pode controlar as expressões e movimentos do personagem especificando tags de emoção. Exemplo: [neutral]Bom dia![happy]Bom trabalho hoje!",
  "CharacterSettingsPrompt": "Prompt de personagem",
  "Characterpreset1": "Predefinição 1",
  "Characterpreset2": "Predefinição 2",
  "Characterpreset3": "Predefinição 3",
  "Characterpreset4": "Predefinição 4",
  "Characterpreset5": "Predefinição 5",
  "CharacterpresetInfo": "Selecionar uma predefinição alterará o prompt do personagem.\nVocê pode usar atalhos com Cmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows).",
  "ChatLog": "Registro de conversas",
  "ClientID": "ID do Cliente",
  "Close": "Fechar",
  "CohereAPIKeyLabel": "Chave API da Cohere",
  "Contact": "Contato",
  "ContactDescription": "Para perguntas sobre este aplicativo, entre em contato pelo endereço de e-mail ou conta do Twitter abaixo.",
  "ContinuousMic": "Entrada de microfone contínua",
  "ContinuousMicActive": "Entrada de microfone contínua ativa",
  "ContinuousMicInfo": "O microfone será reativado automaticamente quando a IA terminar de falar. A entrada será enviada automaticamente após o tempo de silêncio configurado.\nSe não houver reconhecimento de voz após o tempo configurado, o modo de microfone contínuo será desativado automaticamente. Se desejar mantê-lo sempre ATIVADO, defina o timeout de reconhecimento de voz como 0 segundos.",
  "ContinuousMicModeOff": "Modo de entrada de microfone contínua está desativado",
  "ContinuousMicModeOn": "Modo de entrada de microfone contínua está ativado",
  "ConversationContinuityMode": "Modo de continuidade de conversa (beta)",
  "ConversationContinuityModeInfo": "Este é um modo em que a IA tenta continuar a conversa por conta própria quando não há comentários. Atualmente compatível apenas com OpenAI, Anthropic Claude e Google Gemini.",
  "ConversationContinuityModeInfo2": "Como a IA é chamada várias vezes em uma única resposta, os custos de uso da API podem aumentar. Por favor, tenha cuidado.",
  "ConversationContinuityModeInfo3": "Funciona relativamente bem com gpt-4o, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet.",
  "ConversationHistory": "Histórico de conversas",
  "ConversationHistoryInfo": "As últimas {{count}} conversas são mantidas como memória.",
  "ConversationHistoryReset": "Resetar histórico de conversas",
  "Creator": "Informações do criador",
  "CreatorDescription": "Criador: Nike",
  "CustomAPIBody": "Corpo personalizado",
  "CustomAPIBodyInfo": "Insira as informações do corpo a serem incluídas na solicitação de API no formato JSON. As mensagens são incluídas automaticamente.",
  "CustomAPIDescription": "Nota: As mensagens são incluídas automaticamente no corpo da solicitação. No modo de streaming, o servidor deve retornar text/event-stream.",
  "CustomAPIEndpoint": "Endpoint de API personalizado",
  "CustomAPIEndpointInfo": "Insira o URL do endpoint de API para enviar solicitações POST.",
  "CustomAPIHeaders": "Cabeçalhos personalizados",
  "CustomAPIHeadersInfo": "Insira as informações de cabeçalho a serem incluídas na solicitação de API no formato JSON.",
  "CustomAPIStream": "Modo de streaming",
  "CustomAPIStreamForced": "Atualmente, o modo de streaming está sempre ativado.",
  "CustomVoiceTextPlaceholder": "Digite o texto que deseja ouvir",
  "DeepSeekAPIKeyLabel": "Chave API da DeepSeek",
  "DefaultBackground": "Fundo padrão",
  "Description": "Sobre o aplicativo",
  "DifyAPIKeyLabel": "Chave API da Dify",
  "DifyInfo": "O Dify suporta apenas tipos de chatbot ou agente.",
  "DifyInfo2": "O comprimento do histórico de conversas depende das configurações do chatbot Dify.",
  "DifyInfo3": "Exemplo: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Ao usar o Dify, este prompt do sistema não será usado. Configure-o no chatbot Dify.",
  "Documentation": "Documentação",
  "DocumentationDescription": "Instruções detalhadas e tutoriais sobre o AITuberKit estão disponíveis no URL abaixo.",
  "DontShowIntroductionNextTime": "Não mostrar este diálogo na próxima vez",
  "DragToReorder": "Arraste para reordenar",
  "EditSlideScripts": "セリフ編集",
  "ElevenLabsApiKey": "Chave API do ElevenLabs",
  "ElevenLabsInfo": "Usando a API ElevenLabs. Suporta vários idiomas. Obtenha a chave API no URL abaixo.",
  "ElevenLabsVoiceId": "ID de voz do ElevenLabs",
  "ElevenLabsVoiceIdInfo": "Selecione o ID de voz no URL abaixo.",
  "EnglishToJapanese": "Ler palavras em inglês em japonês",
  "EnterPresetQuestion": "Insira uma pergunta",
  "EnterURL": "Inserir URL",
  "EnterYourQuestion": "Digite o que deseja perguntar",
  "Errors": {
    "AIAPIError": "Ocorreu um erro durante a execução da API de IA",
    "AIInvalidProperty": "Os valores de configuração do serviço de IA estão incorretos",
    "CustomAPIError": "Ocorreu um erro na API personalizada",
    "EmptyAPIKey": "A chave API não está configurada",
    "EmptyLocalLLMURL": "O URL do LLM local não está configurado",
    "InvalidAIService": "O serviço de IA selecionado está incorreto",
    "InvalidJSON": "O formato JSON está incorreto",
    "LocalLLMAPIError": "Ocorreu um erro na API do LLM local",
    "LocalLLMConnectionError": "Não foi possível conectar ao servidor LLM local",
    "LocalLLMError": "Ocorreu um erro no LLM local",
    "LocalLLMNotFound": "Endpoint do LLM local não encontrado",
    "LocalLLMStreamError": "Ocorreu um erro no processamento de stream do LLM local",
    "MethodNotAllowed": "A solicitação é inadequada",
    "TTSServiceError": "Ocorreu um erro no serviço TTS {{serviceName}}: {{message}}",
    "UnexpectedError": "Ocorreu um erro desconhecido"
  },
  "ExternalLinkageMode": "Modo de integração externa (beta)",
  "FireworksAPIKeyLabel": "Chave API da Fireworks",
  "GSVITTSBatchSize": "Tamanho do lote GSVI TTS (1 ~ 100, valores maiores aumentam a velocidade de inferência, mas valores muito altos podem esgotar a memória)",
  "GSVITTSInfo": "Configurações do GSVI TTS",
  "GSVITTSModelID": "ID do modelo GSVI TTS",
  "GSVITTSServerUrl": "URL do servidor GSVI TTS",
  "GSVITTSSpeechRate": "Velocidade de fala (0.5 ~ 2.0, valores maiores são mais rápidos)",
  "GoogleAPIKeyLabel": "Chave API do Google Gemini",
  "GoogleTTSInfo": "Usando Google Cloud Text-to-Speech. Suporta vários idiomas.",
  "GroqAPIKeyLabel": "Chave API da Groq",
  "GroqInfo": "A API Groq é acessada diretamente do navegador.",
  "IncludeSystemMessages": "Incluir mensagens do sistema",
  "IncludeTimestampInUserMessage": "Incluir timestamp nas mensagens do usuário",
  "IncludeTimestampInUserMessageInfo": "Incluir um timestamp nas mensagens do usuário permite que a IA gere respostas considerando o tempo.\nIncluir a seguinte frase no prompt do sistema:\n\n\"As entradas do usuário podem ser solicitadas com [timestamp]. Isto representa o horário UTC no momento da solicitação, então por favor considere este horário ao gerar respostas.\"",
  "InitialSpeechTimeout": "Timeout de reconhecimento de voz",
  "InitialSpeechTimeoutInfo": "Define o tempo de espera após o início do reconhecimento de voz até que a primeira fala seja detectada. Se nenhuma fala for detectada dentro deste tempo, o reconhecimento de voz será automaticamente interrompido.\nSe definido como 0 segundos, o tempo de espera será ilimitado.",
  "InputAudio": "Áudio",
  "InputText": "Texto",
  "KoeiromapInfo": "Usando a API Koeiromap da Koemotion. Suporta apenas japonês. Para mais detalhes, consulte abaixo.",
  "Language": "Configurações de idioma",
  "LanguageChoice": "Seleção de idioma",
  "LanguageModelURL": "Selecione o modelo de linguagem no URL abaixo.",
  "ListeningContinuously": "Aguardando entrada de voz...",
  "Live2D": {
    "EmotionInfo": "Várias emoções podem ser especificadas separadas por vírgulas. Se várias forem especificadas, uma será selecionada aleatoriamente.\nOs valores padrão correspondem aos modelos fornecidos com o AITuberKit. Se estiver usando um modelo original, insira valores que correspondam ao seu modelo.\nApós a conclusão da conversa, a expressão \"normal\" será exibida.",
    "Emotions": "Configurações de expressões",
    "FileInfo": "Coloque a pasta do modelo Live2D que deseja usar em public/live2d. Um arquivo model3.json deve existir diretamente nesta pasta.\nSe não aparecer nas opções, recarregue a tela ou verifique se o caminho da pasta está correto.",
    "Info": "Você pode especificar emoções e movimentos.\nCada emoção é controlada pelo prompt. Para mais detalhes, consulte \"Configurações de IA => Configurações de personagem\".",
    "MotionGroups": "Configurações de grupos de movimento",
    "MotionGroupsInfo": "Os movimentos são selecionados aleatoriamente do grupo selecionado.\nAssim como nas configurações de expressão, ajuste de acordo com seu próprio modelo.\nO \"Em espera\" é o movimento exibido após a conclusão da conversa.",
    "SelectMotionGroup": "Selecionar grupo de movimento",
    "angryEmotions": "Raiva",
    "angryMotionGroup": "Raiva",
    "happyEmotions": "Feliz",
    "happyMotionGroup": "Feliz",
    "idleMotionGroup": "Em espera",
    "neutralEmotions": "Normal",
    "neutralMotionGroup": "Normal",
    "relaxedEmotions": "Relaxado",
    "relaxedMotionGroup": "Relaxado",
    "sadEmotions": "Triste",
    "sadMotionGroup": "Triste",
    "surprisedEmotions": "Surpreso",
    "surprisedMotionGroup": "Surpreso"
  },
  "LocalLLM": "LLM local",
  "LocalLLMInfo": "O servidor LLM local precisa estar em execução.",
  "LocalLLMInfo2": "Insira o URL do LLM local (incluindo o número da porta) e o nome do modelo.",
  "LocalStorageReset": "Resetar configurações",
  "LocalStorageResetButton": "Resetar configurações",
  "LocalStorageResetInfo": "Se as variáveis de ambiente estiverem definidas, esses valores terão prioridade. A página será recarregada.",
  "LogSettings": "Histórico de conversas",
  "MaxPastMessages": "Número de mensagens anteriores a manter",
  "MaxTokens": "Número máximo de tokens",
  "MaxTokensInfo": "O número máximo de tokens varia dependendo do modelo de IA em uso. Verifique as especificações de cada modelo.",
  "MessageReceiver": "Aceitar instruções externas",
  "MessageReceiverDescription": "Você pode instruir o personagem de IA a falar externamente usando a API.",
  "Milliseconds": "milissegundos",
  "MistralAIAPIKeyLabel": "Chave API da MistralAI",
  "NijiVoiceActorId": "ID do locutor",
  "NijiVoiceApiKey": "Chave API do Niji Voice",
  "NijiVoiceEmotionalLevel": "Nível emocional",
  "NijiVoiceInfo": "Usando a API Niji Voice. Suporta apenas japonês. Obtenha a chave API no URL abaixo.",
  "NijiVoiceSoundDuration": "Duração do som",
  "NijiVoiceSpeed": "Velocidade de fala",
  "NoSpeechTimeout": "Timeout de detecção de silêncio",
  "NoSpeechTimeoutInfo": "Define o tempo após o qual a entrada será automaticamente finalizada se o silêncio continuar durante a entrada de voz.\nSe definido como 0 segundos, desativa o envio automático por detecção de silêncio.",
  "NotConnectedToExternalAssistant": "Não conectado a um assistente externo.",
  "OpenAIAPIKeyLabel": "Chave API da OpenAI",
  "OpenAITTSInfo": "Usando OpenAI. Suporta vários idiomas. Se você selecionou OpenAI como serviço de IA, não precisa configurar a chave API abaixo.",
  "OpenAITTSModel": "Modelo",
  "OpenAITTSSpeed": "Velocidade de fala",
  "OpenAITTSVoice": "Tipo de voz",
  "OpenSendMessagePage": "Abrir página de envio de mensagens",
  "OpenVRM": "Abrir VRM",
  "OtherSettings": "Outros",
  "PdfConvertButton": "Converter PDF para slides",
  "PdfConvertDescription": "Converte PDFs em dados para o modo de apresentação. Disponível apenas quando o serviço de IA selecionado é OpenAI, Anthropic Claude ou Google Gemini.",
  "PdfConvertError": "Falha na conversão",
  "PdfConvertFileUpload": "Selecionar arquivo PDF",
  "PdfConvertFolderName": "Nome da pasta para salvar",
  "PdfConvertLabel": "Conversão de slides PDF",
  "PdfConvertLoading": "Convertendo...",
  "PdfConvertModelSelect": "Selecionar modelo",
  "PdfConvertSubmitError": "Verifique se o arquivo PDF, o nome da pasta e a chave API estão configurados",
  "PdfConvertSuccess": "Conversão concluída",
  "PerplexityAPIKeyLabel": "Chave API da Perplexity",
  "PleaseSelectSlide": "スライドを選択してください",
  "PresetQuestions": "Perguntas predefinidas",
  "PresetQuestionsInfo": "Você pode criar e registrar vários padrões de perguntas previamente. As perguntas registradas são exibidas como botões na interface do usuário e, quando clicadas, são definidas no campo de entrada do chat.",
  "RealtimeAPIMode": "Modo de API em tempo real",
  "RealtimeAPIModeContentType": "Tipo de envio",
  "RealtimeAPIModeVoice": "Tipo de voz",
  "RepositoryURL": "URL do repositório:",
  "SearchGrounding": "Usar função de pesquisa",
  "SearchGroundingDescription": "Ao usar a função multimodal, a função de pesquisa é automaticamente desativada.",
  "Select": "Selecione",
  "SelectAIService": "Selecionar serviço de IA",
  "SelectModel": "Selecionar modelo",
  "SelectedSlideDocs": "Slides a serem usados",
  "SendMessage": {
    "aiGenerateDescription": "A IA gerará uma resposta a partir da mensagem enviada e fará o personagem de IA falar essa resposta. Se enviar várias mensagens, elas serão processadas em ordem.\nO modelo de IA e o modelo de voz usados serão os selecionados nas configurações do AITuberKit.\nVocê pode escolher usar o prompt do sistema do AITuberKit ou um prompt do sistema personalizado.\nPara carregar o histórico de conversas anteriores, inclua a string [conversation_history] em qualquer posição no prompt do sistema ou na mensagem do usuário.",
    "aiGenerateTitle": "Gerar resposta com IA antes de fazer o personagem falar",
    "directSendDescription": "Você pode fazer o personagem de IA falar exatamente o que você enviar. Se enviar várias mensagens, elas serão processadas em ordem.\nO modelo de voz usado será o selecionado nas configurações do AITuberKit.",
    "directSendTitle": "Fazer o personagem de IA falar diretamente",
    "title": "Adaptador externo do AITuberKit",
    "useCurrentSystemPrompt": "Usar o prompt do sistema do AITuberKit",
    "userInputDescription": "A mensagem enviada será processada da mesma forma que se fosse inserida no formulário de entrada do AITuberKit. Se enviar várias mensagens, elas serão processadas em ordem.\nO modelo de IA e o modelo de voz usados serão os selecionados nas configurações do AITuberKit.\nO prompt do sistema e o histórico de conversas usados serão os valores do AITuberKit.",
    "userInputTitle": "Enviar entrada do usuário"
  },
  "ShowAssistantText": "Mostrar campo de resposta",
  "ShowCharacterName": "Mostrar nome do personagem no campo de resposta",
  "ShowCharacterPresetMenu": "Mostrar botão do menu de predefinições de personagem",
  "ShowControlPanel": "Mostrar painel de controle",
  "ShowControlPanelInfo": "A tela de configurações pode ser exibida com Cmd + . (Mac) / Ctrl + . (Windows).\nSe estiver usando um smartphone, também é possível manter pressionado o canto superior esquerdo da tela (por cerca de 1 segundo).",
  "ShowSilenceProgressBar": "Mostrar barra de progresso de detecção de silêncio",
  "SlideMode": "Modo de apresentação",
  "SlideModeDescription": "Este é um modo em que a IA apresenta slides automaticamente. Está disponível apenas quando o serviço de IA selecionado é OpenAI, Anthropic Claude ou Google Gemini.",
  "SlideSettings": "Configurações de apresentação",
  "SourceCodeDescription1": "O código-fonte deste aplicativo está disponível no GitHub. Você pode modificá-lo livremente.",
  "SourceCodeDescription2": "Para uso comercial, consulte o README no mesmo repositório.",
  "SpeakerSelection": "Seleção de tipo de voz",
  "SpeechInputSettings": "Configurações de entrada de voz",
  "SpeechRecognitionMode": "Modo de reconhecimento de voz",
  "SpeechRecognitionModeDisabledInfo": "Quando o modo de áudio está ativado, apenas o reconhecimento de voz do navegador pode ser usado.\nAlém disso, no modo de API em tempo real, apenas o reconhecimento de voz do navegador está disponível, e a função de timeout de reconhecimento de voz é desativada.",
  "SpeechRecognitionModeInfo": "Você pode selecionar o modo de reconhecimento de voz.\n\"Padrão do navegador\" usa o reconhecimento de voz integrado ao navegador. \"OpenAI TTS\" usa a API Text to Speech da OpenAI.\nGeralmente, \"Padrão do navegador\" é recomendado, pois tem maior precisão e velocidade de reconhecimento. No entanto, se estiver usando um navegador como Firefox que não suporta a WebSpeech API, selecione \"OpenAI TTS\".",
  "StatusOff": "Status: DESATIVADO",
  "StatusOn": "Status: ATIVADO",
  "StyleBeatVITS2ApiKey": "Chave API",
  "StyleBeatVITS2Length": "Velocidade de fala",
  "StyleBeatVITS2ModelID": "ID do modelo",
  "StyleBeatVITS2SdpRatio": "Proporção de mistura SDP/DP",
  "StyleBeatVITS2ServerURL": "URL do servidor",
  "StyleBeatVITS2Style": "Estilo",
  "StyleBertVITS2Info": "Usando Style-Bert-VITS2. Suporta apenas japonês, inglês e chinês. Se usar a API local, você precisa baixar e iniciar o aplicativo adequado ao seu ambiente no site abaixo. Configure a chave API, se necessário.",
  "SyntheticVoiceEngineChoice": "Escolha do motor de voz sintética",
  "TechnologyIntroduction": "Introdução à tecnologia",
  "TechnologyIntroductionDescription1": "Este aplicativo foi criado modificando o <b>ChatVRM</b> da pixiv. O código-fonte original pode ser encontrado",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "Para exibição e manipulação de modelos 3D, usamos",
  "TechnologyIntroductionDescription4": ", para geração de conversas, usamos",
  "TechnologyIntroductionDescription5": "e vários outros LLMs, e para síntese de voz, usamos",
  "TechnologyIntroductionDescription6": "e vários outros TTS. Para mais detalhes, consulte este",
  "TechnologyIntroductionDescription7": ".",
  "TechnologyIntroductionLink1": "aqui",
  "TechnologyIntroductionLink2": "artigo explicativo",
  "Temperature": "Temperatura",
  "TestSelectedVoice": "Reproduzir",
  "TestVoice": "Testar voz",
  "TestVoiceSettings": "Teste de voz",
  "Toasts": {
    "FirefoxNotSupported": "Firefoxではこの機能はサポートされていません",
    "FunctionExecuting": "{{funcName}}を実行しています",
    "FunctionExecutionFailed": "{{funcName}}の実行に失敗しました",
    "NoSpeechDetected": "音声が検出されませんでした。",
    "PresetSwitching": "{{presetName}}に切り替わりました。",
    "SpeechRecognitionError": "音声認識エラーが発生しました",
    "UsingTool": "{{toolName}}を使用中",
    "WebSocketConnectionAttempt": "WebSocket接続を試みています...",
    "WebSocketConnectionClosed": "WebSocket接続が閉じられました",
    "WebSocketConnectionError": "WebSocket接続にエラーが発生しました",
    "WebSocketConnectionSuccess": "WebSocket接続に成功しました",
    "WhisperError": "Whisperによる音声認識でエラーが発生しました"
  },
  "UpdateRealtimeAPISettings": "Atualizar configurações de API em tempo real",
  "UpdateRealtimeAPISettingsInfo": "Ao atualizar a chave API, Azure Endpoint, tipo de voz, modelo ou prompt do sistema, pressione o botão de atualização para iniciar uma nova sessão WebSocket.",
  "UpdateSpeakerList": "Atualizar lista de locutores",
  "UploadBackground": "Carregar imagem de fundo",
  "UseVideoAsBackground": "Usar tela compartilhada ou webcam como fundo",
  "UsingAivisSpeech": "Usar AivisSpeech",
  "UsingAzureTTS": "Usar Azure OpenAI",
  "UsingElevenLabs": "Usar ElevenLabs",
  "UsingGSVITTS": "Usar GSVI TTS",
  "UsingGoogleTTS": "Usar Google Text-to-Speech",
  "UsingKoeiromap": "Usar Koeiromap",
  "UsingNijiVoice": "Usar Niji Voice",
  "UsingOpenAITTS": "Usar OpenAI",
  "UsingStyleBertVITS2": "Usar Style-Bert-VITS2",
  "UsingVoiceVox": "Usar VOICEVOX",
  "VoiceAdjustment": "Ajuste de voz",
  "VoiceEngineInstruction": "Selecione o motor de síntese de voz que deseja usar.",
  "VoiceSettings": "Configurações de voz sintética",
  "VoiceVoxInfo": "Usando VOICEVOX. Suporta apenas japonês. Como usa a API local, você precisa baixar e iniciar o aplicativo adequado ao seu ambiente no site abaixo.",
  "VoicevoxIntonation": "Entonação",
  "VoicevoxPitch": "Tom",
  "VoicevoxServerUrl": "URL do servidor VOICEVOX",
  "VoicevoxSpeed": "Velocidade de fala",
  "WhisperSpeechRecognition": "Usar reconhecimento de voz OpenAI TTS",
  "WhisperTranscriptionModel": "Modelo de transcrição",
  "WhisperTranscriptionModelInfo": "Você pode selecionar o modelo usado para reconhecimento de voz. Modelos mais avançados oferecem maior precisão, mas podem ter custos de API mais altos.",
  "YoutubeAPIKey": "Chave API do YouTube",
  "YoutubeInfo": "Comentários iniciados com '#' serão ignorados.",
  "YoutubeLiveID": "ID do YouTube Live",
  "YoutubeMode": "Modo YouTube",
  "YoutubeSettings": "Configurações do YouTube"
}
