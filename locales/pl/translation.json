{
  "AISettings": "Ustawienia AI",
  "APIKeyInstruction": "Klucze API można uzyskać z poniższego linku. Proszę wprowadzić uzyskany klucz API w formularzu.",
  "APIKeyNotEntered": "Nie wprowadzono klucza API.",
  "AboutThisApplication": "O tej aplikacji",
  "AboutThisApplicationDescription": "Możesz cieszyć się rozmową z postacią 3D tylko za pomocą przeglądarki internetowej, wykorzystując mikrofon, wprowadzanie tekstu i syntezę głosu. Możesz również zmienić postać (VRM), ustawienia osobowości i dostosować głos.<br />Ustawienia można zmienić za pomocą przycisku menu w lewym górnym rogu.",
  "AboutThisApplicationDescription2": "W AITuberKit możesz cieszyć się rozmową z postacią AI tylko za pomocą przeglądarki internetowej. Sprawdź poszczególne ustawienia, aby zmienić postać, ustawienia osobowości i dostosować głos.",
  "AivisSpeechInfo": "Używamy AivisSpeech. Obsługuje tylko język japoński. Ponieważ używa lokalnego API, musisz pobrać i uruchomić aplikację odpowiednią dla swojego środowiska z poniższej strony.",
  "AivisSpeechIntonation": "Intonacja",
  "AivisSpeechPitch": "Wysokość głosu",
  "AivisSpeechServerUrl": "URL serwera AivisSpeech",
  "AivisSpeechSpeaker": "Głos",
  "AivisSpeechSpeed": "Prędkość mowy",
  "AnswerGenerating": "Generowanie odpowiedzi",
  "AnthropicAPIKeyLabel": "Klucz API Anthropic",
  "AudioMode": "Tryb audio",
  "AuthFileInstruction": "Wymagany jest klucz API lub plik JSON do uwierzytelniania. Uzyskaj go z poniższej strony i umieść plik JSON jako credentials.json w głównym folderze repozytorium.",
  "AzureAPIKeyLabel": "Klucz API Azure OpenAI",
  "AzureAPIURL": "URL API Azure OpenAI",
  "AzureEndpoint": "Endpoint Azure",
  "AzureTTSInfo": "Używamy Azure OpenAI. Obsługuje wiele języków.",
  "BackgroundImage": "Obraz tła",
  "BackgroundSettings": "Ustawienia tła",
  "BackgroundSettingsDescription": "Możesz przesłać i wybrać obraz tła dla aplikacji.",
  "BasedSettings": "Ustawienia podstawowe",
  "BrowserSpeechRecognition": "Użyj standardowego rozpoznawania mowy przeglądarki",
  "CannotUseParameters": "Gdy tryb API w czasie rzeczywistym lub tryb audio jest włączony, nie można określić parametrów Temperature i Max Tokens.",
  "CannotUseVoice": "Gdy tryb API w czasie rzeczywistym lub tryb audio jest włączony,\nustawienia syntezy głosu nie są potrzebne.",
  "ChangeBackgroundImage": "Zmień obraz tła",
  "CharacterModelInfo": "W zależności od modelu, początkowe ładowanie może potrwać dłużej.",
  "CharacterModelLabel": "Model postaci",
  "CharacterName": "Nazwa postaci",
  "CharacterSettings": "Ustawienia postaci",
  "CharacterSettingsInfo": "Ta wartość jest ustawiana jako prompt systemowy.\nKorzystając z początkowego promptu, możesz kontrolować wyrażenia i ruchy postaci, określając tagi emocji. Przykład: [neutral]Dzień dobry![happy]Dobrej pracy na dziś!",
  "CharacterSettingsPrompt": "Prompt postaci",
  "Characterpreset1": "Preset 1",
  "Characterpreset2": "Preset 2",
  "Characterpreset3": "Preset 3",
  "Characterpreset4": "Preset 4",
  "Characterpreset5": "Preset 5",
  "CharacterpresetInfo": "Wybór presetu zmieni prompt postaci.\nMożna używać skrótów Cmd + Shift + 1~5 (Mac) / Ctrl + Shift + 1~5 (Windows).",
  "ChatLog": "Dziennik rozmów",
  "ClientID": "ID klienta",
  "Close": "Zamknij",
  "CohereAPIKeyLabel": "Klucz API Cohere",
  "Contact": "Kontakt",
  "ContactDescription": "W przypadku pytań dotyczących tej aplikacji, skontaktuj się za pomocą poniższego adresu e-mail lub konta Twitter.",
  "ContinuousMic": "Ciągłe wejście mikrofonu",
  "ContinuousMicActive": "Ciągłe wejście mikrofonu aktywne",
  "ContinuousMicInfo": "Automatycznie wznawia wejście mikrofonu po zakończeniu wypowiedzi AI. Automatycznie wysyła po upływie ustawionego czasu ciszy.\nJeśli rozpoznawanie mowy nie nastąpi przed upływem ustawionego czasu, ciągłe wejście mikrofonu zostanie automatycznie wyłączone. Jeśli chcesz, aby zawsze było włączone, ustaw limit czasu rozpoznawania mowy na 0 sekund.",
  "ContinuousMicModeOff": "Tryb ciągłego wejścia mikrofonu wyłączony",
  "ContinuousMicModeOn": "Tryb ciągłego wejścia mikrofonu włączony",
  "ConversationContinuityMode": "Tryb ciągłości rozmowy (wersja beta)",
  "ConversationContinuityModeInfo": "Tryb, w którym AI próbuje kontynuować rozmowę, gdy nie ma komentarzy. Obecnie obsługiwane są tylko OpenAI, Anthropic Claude i Google Gemini.",
  "ConversationContinuityModeInfo2": "Ponieważ LLM jest wywoływany wielokrotnie w jednej odpowiedzi, koszty API mogą wzrosnąć. Proszę o ostrożność.",
  "ConversationContinuityModeInfo3": "Działa relatywnie stabilnie z gpt-4o, gpt-4-turbo, claude-3-opus, claude-3.5-sonnet.",
  "ConversationHistory": "Historia rozmowy",
  "ConversationHistoryInfo": "Ostatnie {{count}} wiadomości rozmowy są przechowywane jako pamięć.",
  "ConversationHistoryReset": "Resetuj historię rozmowy",
  "Creator": "Informacje o twórcy",
  "CreatorDescription": "Twórca: Nike",
  "CustomAPIBody": "Niestandardowy treść",
  "CustomAPIBodyInfo": "Wprowadź informacje treści, które mają być zawarte w żądaniu API, w formacie JSON. Wiadomości są automatycznie dołączane.",
  "CustomAPIDescription": "Uwaga: Wiadomości są automatycznie dołączane do treści żądania. W trybie strumieniowym serwer musi zwracać text/event-stream.",
  "CustomAPIEndpoint": "Niestandardowy punkt końcowy API",
  "CustomAPIEndpointInfo": "Wprowadź URL punktu końcowego API, do którego będą wysyłane żądania POST.",
  "CustomAPIHeaders": "Niestandardowe nagłówki",
  "CustomAPIHeadersInfo": "Wprowadź informacje nagłówka, które mają być zawarte w żądaniu API, w formacie JSON.",
  "CustomAPIStream": "Tryb strumieniowy",
  "CustomAPIStreamForced": "Tryb strumieniowy jest obecnie zawsze włączony.",
  "CustomVoiceTextPlaceholder": "Wprowadź tekst do przetestowania",
  "DeepSeekAPIKeyLabel": "Klucz API DeepSeek",
  "DefaultBackground": "Domyślne tło",
  "Description": "O aplikacji",
  "DifyAPIKeyLabel": "Klucz API Dify",
  "DifyInfo": "Dify obsługuje tylko chatboty lub typy agentów.",
  "DifyInfo2": "Długość historii rozmowy zależy od ustawień chatbota Dify.",
  "DifyInfo3": "Przykład: https://api.dify.ai/v1, http://localhost:80/v1",
  "DifyInstruction": "Gdy używasz Dify, ten prompt systemowy nie jest używany. Ustaw go w chatbocie Dify.",
  "Documentation": "Dokumentacja",
  "DocumentationDescription": "Szczegółowe instrukcje i tutoriale dotyczące AITuberKit można znaleźć pod poniższym URL.",
  "DontShowIntroductionNextTime": "Nie pokazuj tego dialogu następnym razem",
  "DragToReorder": "Przeciągnij, aby zmienić kolejność",
  "EditSlideScripts": "セリフ編集",
  "ElevenLabsApiKey": "Klucz API ElevenLabs",
  "ElevenLabsInfo": "Używamy API ElevenLabs. Obsługuje wiele języków. Uzyskaj klucz API z poniższego URL.",
  "ElevenLabsVoiceId": "ID głosu ElevenLabs",
  "ElevenLabsVoiceIdInfo": "Wybierz ID głosu z poniższego URL.",
  "EnglishToJapanese": "Czytaj angielskie słowa po japońsku",
  "EnterPresetQuestion": "Wprowadź pytanie",
  "EnterURL": "Wprowadź URL",
  "EnterYourQuestion": "Wpisz, o co chcesz zapytać",
  "Errors": {
    "AIAPIError": "Wystąpił błąd podczas wykonywania API AI",
    "AIInvalidProperty": "Wartość ustawienia usługi AI jest nieprawidłowa",
    "CustomAPIError": "Wystąpił błąd w niestandardowym API",
    "EmptyAPIKey": "Klucz API nie jest ustawiony",
    "EmptyLocalLLMURL": "URL lokalnego LLM nie jest ustawiony",
    "InvalidAIService": "Wybrana usługa AI jest nieprawidłowa",
    "InvalidJSON": "Format JSON jest nieprawidłowy",
    "LocalLLMAPIError": "Wystąpił błąd w API lokalnego LLM",
    "LocalLLMConnectionError": "Nie można połączyć się z serwerem lokalnego LLM",
    "LocalLLMError": "Wystąpił błąd w lokalnym LLM",
    "LocalLLMNotFound": "Nie znaleziono punktu końcowego lokalnego LLM",
    "LocalLLMStreamError": "Wystąpił błąd w przetwarzaniu strumienia lokalnego LLM",
    "MethodNotAllowed": "Żądanie nie jest odpowiednie",
    "TTSServiceError": "Wystąpił błąd w usłudze TTS {{serviceName}}: {{message}}",
    "UnexpectedError": "Wystąpił nieznany błąd"
  },
  "ExternalLinkageMode": "Tryb połączenia zewnętrznego (wersja beta)",
  "FireworksAPIKeyLabel": "Klucz API Fireworks",
  "GSVITTSBatchSize": "Rozmiar partii GSVI TTS (1 ~ 100, większa wartość przyspiesza wnioskowanie, ale zbyt duża może wyczerpać pamięć)",
  "GSVITTSInfo": "Ustawienia GSVI TTS",
  "GSVITTSModelID": "ID modelu GSVI TTS",
  "GSVITTSServerUrl": "URL serwera GSVI TTS",
  "GSVITTSSpeechRate": "Prędkość mowy (0.5 ~ 2.0, większa wartość oznacza szybciej)",
  "GoogleAPIKeyLabel": "Klucz API Google Gemini",
  "GoogleTTSInfo": "Używamy Google Cloud Text-to-Speech. Obsługuje wiele języków.",
  "GroqAPIKeyLabel": "Klucz API Groq",
  "GroqInfo": "API Groq jest dostępne bezpośrednio z przeglądarki.",
  "IncludeSystemMessages": "Uwzględnij wiadomości systemowe",
  "IncludeTimestampInUserMessage": "Dołącz znacznik czasu do wypowiedzi użytkownika",
  "IncludeTimestampInUserMessageInfo": "Dołączenie znacznika czasu do wypowiedzi użytkownika pozwala AI generować odpowiedzi z uwzględnieniem czasu.\nDodaj następujący tekst do promptu systemowego:\n\n\"Jeśli dane wejściowe użytkownika zawierają [timestamp], reprezentuje to czas w strefie czasowej UTC w momencie żądania, więc proszę wygenerować odpowiedź z uwzględnieniem tego czasu.\"",
  "InitialSpeechTimeout": "Limit czasu rozpoznawania mowy",
  "InitialSpeechTimeoutInfo": "Ustaw czas oczekiwania na wykrycie pierwszej wypowiedzi po rozpoczęciu rozpoznawania głosu. Jeśli w tym czasie nie zostanie wykryta żadna wypowiedź, rozpoznawanie głosu zostanie automatycznie zatrzymane.\nUstawienie na 0 sekund oznacza nieograniczony czas oczekiwania.",
  "InputAudio": "Głos",
  "InputText": "Tekst",
  "KoeiromapInfo": "Używamy API Koeiromap Koemotion. Obsługuje tylko język japoński. Więcej informacji poniżej.",
  "Language": "Ustawienia języka",
  "LanguageChoice": "Wybór języka",
  "LanguageModelURL": "Wybierz model języka z poniższego URL.",
  "ListeningContinuously": "Oczekiwanie na wejście głosowe...",
  "Live2D": {
    "EmotionInfo": "Emocje można określić oddzielając je przecinkami. W przypadku wielu określeń jedna zostanie wybrana losowo.\nWartości domyślne są przeznaczone dla modeli dostarczanych z AITuberKit. Jeśli używasz własnego modelu, wprowadź wartości odpowiednie dla swojego modelu.\nPo zakończeniu rozmowy wyświetlane jest wyrażenie \"normalne\".",
    "Emotions": "Ustawienia emocji",
    "FileInfo": "Umieść folder z modelem Live2D, którego chcesz użyć, w public/live2d. Plik model3.json musi istnieć bezpośrednio w tym folderze.\nJeśli nie pojawia się w opcjach, odśwież stronę lub sprawdź, czy ścieżka folderu jest poprawna.",
    "Info": "Możesz określić emocje i ruchy.\nKażda emocja jest kontrolowana przez prompt. Szczegóły znajdziesz w \"Ustawienia AI => Ustawienia postaci\".",
    "MotionGroups": "Ustawienia grup ruchów",
    "MotionGroupsInfo": "Ruchy są losowo wybierane z wybranej grupy.\nPodobnie jak w przypadku ustawień emocji, dostosuj do własnego modelu.\n\"Bezczynność\" to ruch wyświetlany po zakończeniu rozmowy.",
    "SelectMotionGroup": "Wybierz grupę ruchów",
    "angryEmotions": "Złe",
    "angryMotionGroup": "Złe",
    "happyEmotions": "Szczęśliwe",
    "happyMotionGroup": "Szczęśliwe",
    "idleMotionGroup": "Bezczynność",
    "neutralEmotions": "Normalne",
    "neutralMotionGroup": "Normalne",
    "relaxedEmotions": "Zrelaksowane",
    "relaxedMotionGroup": "Zrelaksowane",
    "sadEmotions": "Smutne",
    "sadMotionGroup": "Smutne",
    "surprisedEmotions": "Zaskoczone",
    "surprisedMotionGroup": "Zaskoczone"
  },
  "LocalLLM": "Lokalny LLM",
  "LocalLLMInfo": "Wymagane jest uruchomienie serwera lokalnego LLM.",
  "LocalLLMInfo2": "Wprowadź URL lokalnego LLM (włącznie z numerem portu) i nazwę modelu.",
  "LocalStorageReset": "Resetuj ustawienia",
  "LocalStorageResetButton": "Resetuj ustawienia",
  "LocalStorageResetInfo": "Jeśli ustawione są zmienne środowiskowe, ich wartości mają pierwszeństwo. Strona zostanie ponownie załadowana.",
  "LogSettings": "Historia rozmów",
  "MaxPastMessages": "Liczba przechowywanych poprzednich wiadomości",
  "MaxTokens": "Maksymalna liczba tokenów",
  "MaxTokensInfo": "Maksymalna liczba tokenów różni się w zależności od używanego modelu AI. Sprawdź specyfikację każdego modelu.",
  "MessageReceiver": "Przyjmuj instrukcje z zewnątrz",
  "MessageReceiverDescription": "Możesz instruować postać AI z zewnątrz za pomocą API.",
  "Milliseconds": "milisekundy",
  "MistralAIAPIKeyLabel": "Klucz API MistralAI",
  "NijiVoiceActorId": "ID głosu",
  "NijiVoiceApiKey": "Klucz API NijiVoice",
  "NijiVoiceEmotionalLevel": "Poziom emocji",
  "NijiVoiceInfo": "Używamy API NijiVoice. Obsługuje tylko język japoński. Uzyskaj klucz API z poniższego URL.",
  "NijiVoiceSoundDuration": "Długość dźwięku",
  "NijiVoiceSpeed": "Prędkość mowy",
  "NoSpeechTimeout": "Limit czasu wykrywania ciszy",
  "NoSpeechTimeoutInfo": "Ustaw czas, po którym wejście głosowe zostanie automatycznie zakończone, jeśli wystąpi cisza podczas wprowadzania głosu.\nUstawienie na 0 sekund wyłącza automatyczne wysyłanie po wykryciu ciszy.",
  "NotConnectedToExternalAssistant": "Nie połączono z zewnętrznym asystentem.",
  "OpenAIAPIKeyLabel": "Klucz API OpenAI",
  "OpenAITTSInfo": "Używamy OpenAI. Obsługuje wiele języków. Jeśli wybrano OpenAI jako usługę AI, nie musisz ustawiać poniższego klucza API.",
  "OpenAITTSModel": "Model",
  "OpenAITTSSpeed": "Prędkość mowy",
  "OpenAITTSVoice": "Typ głosu",
  "OpenSendMessagePage": "Otwórz stronę wysyłania wiadomości",
  "OpenVRM": "Otwórz VRM",
  "OtherSettings": "Inne",
  "PdfConvertButton": "Konwertuj PDF na slajdy",
  "PdfConvertDescription": "Konwertuj PDF na dane dla trybu slajdów. Dostępne tylko wtedy, gdy wybrana usługa AI to OpenAI, Anthropic Claude lub Google Gemini.",
  "PdfConvertError": "Konwersja nie powiodła się",
  "PdfConvertFileUpload": "Wybierz plik PDF",
  "PdfConvertFolderName": "Nazwa folderu zapisu",
  "PdfConvertLabel": "Konwersja slajdów PDF",
  "PdfConvertLoading": "Konwersja...",
  "PdfConvertModelSelect": "Wybierz model",
  "PdfConvertSubmitError": "Sprawdź, czy plik PDF, nazwa folderu i klucz API są ustawione",
  "PdfConvertSuccess": "Konwersja zakończona",
  "PerplexityAPIKeyLabel": "Klucz API Perplexity",
  "PleaseSelectSlide": "スライドを選択してください",
  "PresetQuestions": "Wstępnie ustawione pytania",
  "PresetQuestionsInfo": "Możesz tworzyć i rejestrować wiele wzorców pytań. Zarejestrowane pytania będą wyświetlane jako przyciski w interfejsie użytkownika, które po kliknięciu zostaną wprowadzone w polu czatu.",
  "RealtimeAPIMode": "Tryb API w czasie rzeczywistym",
  "RealtimeAPIModeContentType": "Typ transmisji",
  "RealtimeAPIModeVoice": "Typ głosu",
  "RepositoryURL": "URL repozytorium:",
  "SearchGrounding": "Użyj funkcji wyszukiwania",
  "SearchGroundingDescription": "Funkcja wyszukiwania jest automatycznie wyłączana podczas korzystania z funkcji multimodalnych.",
  "Select": "Wybierz",
  "SelectAIService": "Wybierz usługę AI",
  "SelectModel": "Wybierz model",
  "SelectedSlideDocs": "Używane slajdy",
  "SendMessage": {
    "aiGenerateDescription": "AI generuje odpowiedź na podstawie wysłanej wiadomości, a postać AI wypowiada tę odpowiedź. W przypadku wielu wiadomości będą one przetwarzane po kolei.\nModel AI i model głosowy są wybierane w ustawieniach AITuberKit.\nMożesz wybrać, czy chcesz używać promptu systemowego AITuberKit, czy niestandardowego promptu systemowego.\nAby załadować poprzednią historię rozmowy, dołącz ciąg \"[conversation_history]\" w dowolnym miejscu promptu systemowego lub wiadomości użytkownika.",
    "aiGenerateTitle": "Wygenerować odpowiedź przez AI, a następnie kazać postaci mówić",
    "directSendDescription": "Możesz kazać postaci AI mówić dokładnie to, co wysyłasz. W przypadku wielu wiadomości będą one przetwarzane po kolei.\nUżywany jest model głosowy wybrany w ustawieniach AITuberKit.",
    "directSendTitle": "Kazać postaci AI mówić bezpośrednio",
    "title": "Adapter zewnętrzny AITuberKit",
    "useCurrentSystemPrompt": "Użyj promptu systemowego AITuberKit",
    "userInputDescription": "Wysłana wiadomość jest przetwarzana tak samo, jakby została wprowadzona z formularza wejściowego AITuberKit. W przypadku wielu wiadomości będą one przetwarzane po kolei.\nModel AI i model głosowy są wybierane w ustawieniach AITuberKit.\nUżywany jest prompt systemowy i historia rozmowy z AITuberKit.",
    "userInputTitle": "Wysłać dane wejściowe użytkownika"
  },
  "ShowAssistantText": "Pokaż pole odpowiedzi",
  "ShowCharacterName": "Pokaż nazwę postaci w polu odpowiedzi",
  "ShowCharacterPresetMenu": "Pokaż przycisk menu presetów postaci",
  "ShowControlPanel": "Pokaż panel sterowania",
  "ShowControlPanelInfo": "Ekran ustawień można wyświetlić za pomocą Cmd + . (Mac) / Ctrl + . (Windows).\nJeśli korzystasz ze smartfona, możesz także przytrzymać lewy górny róg ekranu (przez około 1 sekundę).",
  "ShowSilenceProgressBar": "Pokaż pasek postępu wykrywania ciszy",
  "SlideMode": "Tryb slajdów",
  "SlideModeDescription": "Tryb, w którym AI automatycznie prezentuje slajdy. Działa tylko wtedy, gdy wybrana usługa AI to OpenAI, Anthropic Claude lub Google Gemini.",
  "SlideSettings": "Ustawienia slajdów",
  "SourceCodeDescription1": "Kod źródłowy tej aplikacji jest publikowany na GitHubie. Możesz swobodnie go modyfikować i zmieniać.",
  "SourceCodeDescription2": "Informacje na temat użytku komercyjnego znajdują się w README w tym repozytorium.",
  "SpeakerSelection": "Wybór typu głosu",
  "SpeechInputSettings": "Ustawienia wejścia głosowego",
  "SpeechRecognitionMode": "Tryb rozpoznawania mowy",
  "SpeechRecognitionModeDisabledInfo": "Gdy tryb audio jest włączony, można używać tylko rozpoznawania mowy przeglądarki.\nPonadto, w trybie API w czasie rzeczywistym można używać tylko rozpoznawania mowy przeglądarki, a funkcja limitu czasu rozpoznawania mowy jest wyłączona.",
  "SpeechRecognitionModeInfo": "Możesz wybrać tryb rozpoznawania mowy.\n\"Standardowy przeglądarkowy\" używa wbudowanego rozpoznawania mowy przeglądarki. \"OpenAI TTS\" używa API Text to Speech OpenAI.\nOgólnie zaleca się \"Standardowy przeglądarkowy\", ponieważ ma wyższą dokładność i szybsze rozpoznawanie. Jednak jeśli używasz przeglądarki, która nie obsługuje WebSpeech API, takiej jak Firefox, wybierz \"OpenAI TTS\".",
  "StatusOff": "Stan: WYŁĄCZONY",
  "StatusOn": "Stan: WŁĄCZONY",
  "StyleBeatVITS2ApiKey": "Klucz API",
  "StyleBeatVITS2Length": "Prędkość mowy",
  "StyleBeatVITS2ModelID": "ID modelu",
  "StyleBeatVITS2SdpRatio": "Stosunek mieszania SDP/DP",
  "StyleBeatVITS2ServerURL": "URL serwera",
  "StyleBeatVITS2Style": "Styl",
  "StyleBertVITS2Info": "Używamy Style-Bert-VITS2. Obsługuje tylko języki japoński, angielski i chiński. Jeśli używasz lokalnego API, musisz pobrać i uruchomić aplikację odpowiednią dla swojego środowiska z poniższej strony. W razie potrzeby ustaw także klucz API.",
  "SyntheticVoiceEngineChoice": "Wybór silnika syntezy głosu",
  "TechnologyIntroduction": "Wprowadzenie technologii",
  "TechnologyIntroductionDescription1": "Ta aplikacja została stworzona poprzez modyfikację <b>ChatVRM</b> firmy pixiv. Oryginalny kod źródłowy można znaleźć",
  "TechnologyIntroductionDescription2": ".",
  "TechnologyIntroductionDescription3": "Do wyświetlania i obsługi modeli 3D używamy",
  "TechnologyIntroductionDescription4": ", do generowania rozmów używamy",
  "TechnologyIntroductionDescription5": "i innych LLM, a do syntezy głosu używamy",
  "TechnologyIntroductionDescription6": "i innych TTS. Szczegółowe informacje można znaleźć w",
  "TechnologyIntroductionDescription7": ".",
  "TechnologyIntroductionLink1": "tutaj",
  "TechnologyIntroductionLink2": "artykule wyjaśniającym",
  "Temperature": "Temperature",
  "TestSelectedVoice": "Odtwórz",
  "TestVoice": "Testuj głos",
  "TestVoiceSettings": "Test głosu",
  "Toasts": {
    "FirefoxNotSupported": "Firefoxではこの機能はサポートされていません",
    "FunctionExecuting": "{{funcName}}を実行しています",
    "FunctionExecutionFailed": "{{funcName}}の実行に失敗しました",
    "NoSpeechDetected": "音声が検出されませんでした。",
    "PresetSwitching": "{{presetName}}に切り替わりました。",
    "SpeechRecognitionError": "音声認識エラーが発生しました",
    "UsingTool": "{{toolName}}を使用中",
    "WebSocketConnectionAttempt": "WebSocket接続を試みています...",
    "WebSocketConnectionClosed": "WebSocket接続が閉じられました",
    "WebSocketConnectionError": "WebSocket接続にエラーが発生しました",
    "WebSocketConnectionSuccess": "WebSocket接続に成功しました",
    "WhisperError": "Whisperによる音声認識でエラーが発生しました"
  },
  "UpdateRealtimeAPISettings": "Aktualizuj ustawienia API w czasie rzeczywistym",
  "UpdateRealtimeAPISettingsInfo": "Po aktualizacji klucza API, endpointu Azure, typu głosu, modelu lub promptu systemowego, naciśnij przycisk aktualizacji, aby rozpocząć nową sesję WebSocket.",
  "UpdateSpeakerList": "Aktualizuj listę głosów",
  "UploadBackground": "Prześlij obraz tła",
  "UseVideoAsBackground": "Użyj udostępnionego ekranu lub kamery internetowej jako tła",
  "UsingAivisSpeech": "Użyj AivisSpeech",
  "UsingAzureTTS": "Użyj Azure OpenAI",
  "UsingElevenLabs": "Użyj ElevenLabs",
  "UsingGSVITTS": "Użyj GSVI TTS",
  "UsingGoogleTTS": "Użyj Google Text-to-Speech",
  "UsingKoeiromap": "Użyj Koeiromap",
  "UsingNijiVoice": "Użyj NijiVoice",
  "UsingOpenAITTS": "Użyj OpenAI",
  "UsingStyleBertVITS2": "Użyj Style-Bert-VITS2",
  "UsingVoiceVox": "Użyj VOICEVOX",
  "VoiceAdjustment": "Regulacja głosu",
  "VoiceEngineInstruction": "Wybierz silnik syntezy głosu do użycia.",
  "VoiceSettings": "Ustawienia syntezy głosu",
  "VoiceVoxInfo": "Używamy VOICEVOX. Obsługuje tylko język japoński. Ponieważ używa lokalnego API, musisz pobrać i uruchomić aplikację odpowiednią dla swojego środowiska z poniższej strony.",
  "VoicevoxIntonation": "Intonacja",
  "VoicevoxPitch": "Wysokość głosu",
  "VoicevoxServerUrl": "URL serwera VOICEVOX",
  "VoicevoxSpeed": "Prędkość mowy",
  "WhisperSpeechRecognition": "Użyj rozpoznawania mowy OpenAI TTS",
  "WhisperTranscriptionModel": "Model transkrypcji",
  "WhisperTranscriptionModelInfo": "Możesz wybrać model używany do rozpoznawania mowy. Bardziej zaawansowane modele oferują wyższą dokładność rozpoznawania, ale mogą wiązać się z wyższymi kosztami API.",
  "YoutubeAPIKey": "Klucz API YouTube",
  "YoutubeInfo": "Komentarze zaczynające się od \"#\" są ignorowane.",
  "YoutubeLiveID": "ID transmisji YouTube Live",
  "YoutubeMode": "Tryb YouTube",
  "YoutubeSettings": "Ustawienia YouTube"
}
